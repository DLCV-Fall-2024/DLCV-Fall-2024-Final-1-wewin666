{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9996528982992017,
  "eval_steps": 500,
  "global_step": 7202,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00027768136063866715,
      "grad_norm": 5.53125,
      "learning_rate": 0.00019997222993612884,
      "loss": 7.2266,
      "step": 1
    },
    {
      "epoch": 0.0027768136063866713,
      "grad_norm": 5.40625,
      "learning_rate": 0.00019972229936128855,
      "loss": 7.6076,
      "step": 10
    },
    {
      "epoch": 0.0055536272127733426,
      "grad_norm": 3.84375,
      "learning_rate": 0.0001994445987225771,
      "loss": 6.0938,
      "step": 20
    },
    {
      "epoch": 0.008330440819160014,
      "grad_norm": 12.4375,
      "learning_rate": 0.0001991668980838656,
      "loss": 5.7469,
      "step": 30
    },
    {
      "epoch": 0.011107254425546685,
      "grad_norm": 4.4375,
      "learning_rate": 0.00019888919744515414,
      "loss": 5.6195,
      "step": 40
    },
    {
      "epoch": 0.013884068031933356,
      "grad_norm": 10.1875,
      "learning_rate": 0.00019861149680644268,
      "loss": 5.4492,
      "step": 50
    },
    {
      "epoch": 0.016660881638320028,
      "grad_norm": 4.125,
      "learning_rate": 0.0001983337961677312,
      "loss": 5.3855,
      "step": 60
    },
    {
      "epoch": 0.0194376952447067,
      "grad_norm": 4.3125,
      "learning_rate": 0.00019805609552901973,
      "loss": 5.168,
      "step": 70
    },
    {
      "epoch": 0.02221450885109337,
      "grad_norm": 4.1875,
      "learning_rate": 0.00019777839489030824,
      "loss": 5.3246,
      "step": 80
    },
    {
      "epoch": 0.024991322457480043,
      "grad_norm": 3.484375,
      "learning_rate": 0.00019750069425159678,
      "loss": 5.2547,
      "step": 90
    },
    {
      "epoch": 0.027768136063866713,
      "grad_norm": 3.8125,
      "learning_rate": 0.00019722299361288532,
      "loss": 5.1668,
      "step": 100
    },
    {
      "epoch": 0.030544949670253386,
      "grad_norm": 3.765625,
      "learning_rate": 0.00019694529297417386,
      "loss": 4.9484,
      "step": 110
    },
    {
      "epoch": 0.033321763276640055,
      "grad_norm": 3.359375,
      "learning_rate": 0.0001966675923354624,
      "loss": 5.0387,
      "step": 120
    },
    {
      "epoch": 0.03609857688302673,
      "grad_norm": 3.15625,
      "learning_rate": 0.0001963898916967509,
      "loss": 5.2391,
      "step": 130
    },
    {
      "epoch": 0.0388753904894134,
      "grad_norm": 4.03125,
      "learning_rate": 0.00019611219105803945,
      "loss": 4.8344,
      "step": 140
    },
    {
      "epoch": 0.04165220409580007,
      "grad_norm": 3.71875,
      "learning_rate": 0.000195834490419328,
      "loss": 4.9672,
      "step": 150
    },
    {
      "epoch": 0.04442901770218674,
      "grad_norm": 3.28125,
      "learning_rate": 0.0001955567897806165,
      "loss": 5.0293,
      "step": 160
    },
    {
      "epoch": 0.047205831308573414,
      "grad_norm": 5.6875,
      "learning_rate": 0.00019527908914190504,
      "loss": 4.766,
      "step": 170
    },
    {
      "epoch": 0.04998264491496009,
      "grad_norm": 3.359375,
      "learning_rate": 0.00019500138850319355,
      "loss": 5.0875,
      "step": 180
    },
    {
      "epoch": 0.05275945852134675,
      "grad_norm": 3.625,
      "learning_rate": 0.0001947236878644821,
      "loss": 5.018,
      "step": 190
    },
    {
      "epoch": 0.055536272127733426,
      "grad_norm": 3.40625,
      "learning_rate": 0.00019444598722577063,
      "loss": 4.7266,
      "step": 200
    },
    {
      "epoch": 0.0583130857341201,
      "grad_norm": 3.984375,
      "learning_rate": 0.00019416828658705917,
      "loss": 4.9031,
      "step": 210
    },
    {
      "epoch": 0.06108989934050677,
      "grad_norm": 3.859375,
      "learning_rate": 0.00019389058594834768,
      "loss": 4.9547,
      "step": 220
    },
    {
      "epoch": 0.06386671294689344,
      "grad_norm": 3.234375,
      "learning_rate": 0.00019361288530963622,
      "loss": 4.8293,
      "step": 230
    },
    {
      "epoch": 0.06664352655328011,
      "grad_norm": 3.625,
      "learning_rate": 0.00019333518467092476,
      "loss": 4.7527,
      "step": 240
    },
    {
      "epoch": 0.06942034015966678,
      "grad_norm": 4.0,
      "learning_rate": 0.0001930574840322133,
      "loss": 4.8379,
      "step": 250
    },
    {
      "epoch": 0.07219715376605346,
      "grad_norm": 4.40625,
      "learning_rate": 0.00019277978339350183,
      "loss": 4.6004,
      "step": 260
    },
    {
      "epoch": 0.07497396737244012,
      "grad_norm": 3.625,
      "learning_rate": 0.00019250208275479035,
      "loss": 4.8473,
      "step": 270
    },
    {
      "epoch": 0.0777507809788268,
      "grad_norm": 3.28125,
      "learning_rate": 0.00019222438211607886,
      "loss": 4.9074,
      "step": 280
    },
    {
      "epoch": 0.08052759458521347,
      "grad_norm": 3.5625,
      "learning_rate": 0.0001919466814773674,
      "loss": 4.6051,
      "step": 290
    },
    {
      "epoch": 0.08330440819160013,
      "grad_norm": 3.09375,
      "learning_rate": 0.00019166898083865594,
      "loss": 4.6219,
      "step": 300
    },
    {
      "epoch": 0.08608122179798681,
      "grad_norm": 3.5625,
      "learning_rate": 0.00019139128019994448,
      "loss": 4.702,
      "step": 310
    },
    {
      "epoch": 0.08885803540437348,
      "grad_norm": 3.484375,
      "learning_rate": 0.000191113579561233,
      "loss": 4.8547,
      "step": 320
    },
    {
      "epoch": 0.09163484901076015,
      "grad_norm": 3.9375,
      "learning_rate": 0.00019083587892252153,
      "loss": 4.7637,
      "step": 330
    },
    {
      "epoch": 0.09441166261714683,
      "grad_norm": 2.921875,
      "learning_rate": 0.00019055817828381007,
      "loss": 4.7602,
      "step": 340
    },
    {
      "epoch": 0.0971884762235335,
      "grad_norm": 3.21875,
      "learning_rate": 0.0001902804776450986,
      "loss": 4.6039,
      "step": 350
    },
    {
      "epoch": 0.09996528982992017,
      "grad_norm": 4.15625,
      "learning_rate": 0.00019000277700638714,
      "loss": 4.6641,
      "step": 360
    },
    {
      "epoch": 0.10274210343630684,
      "grad_norm": 3.34375,
      "learning_rate": 0.00018972507636767566,
      "loss": 4.5691,
      "step": 370
    },
    {
      "epoch": 0.1055189170426935,
      "grad_norm": 3.328125,
      "learning_rate": 0.0001894473757289642,
      "loss": 4.6031,
      "step": 380
    },
    {
      "epoch": 0.10829573064908019,
      "grad_norm": 3.140625,
      "learning_rate": 0.0001891696750902527,
      "loss": 4.5992,
      "step": 390
    },
    {
      "epoch": 0.11107254425546685,
      "grad_norm": 2.796875,
      "learning_rate": 0.00018889197445154125,
      "loss": 4.632,
      "step": 400
    },
    {
      "epoch": 0.11384935786185352,
      "grad_norm": 3.328125,
      "learning_rate": 0.00018861427381282978,
      "loss": 4.6277,
      "step": 410
    },
    {
      "epoch": 0.1166261714682402,
      "grad_norm": 3.8125,
      "learning_rate": 0.0001883365731741183,
      "loss": 4.6465,
      "step": 420
    },
    {
      "epoch": 0.11940298507462686,
      "grad_norm": 4.03125,
      "learning_rate": 0.00018805887253540684,
      "loss": 4.7012,
      "step": 430
    },
    {
      "epoch": 0.12217979868101354,
      "grad_norm": 3.71875,
      "learning_rate": 0.00018778117189669537,
      "loss": 4.5848,
      "step": 440
    },
    {
      "epoch": 0.12495661228740021,
      "grad_norm": 3.859375,
      "learning_rate": 0.0001875034712579839,
      "loss": 4.4754,
      "step": 450
    },
    {
      "epoch": 0.1277334258937869,
      "grad_norm": 3.953125,
      "learning_rate": 0.00018722577061927243,
      "loss": 4.6746,
      "step": 460
    },
    {
      "epoch": 0.13051023950017354,
      "grad_norm": 3.125,
      "learning_rate": 0.00018694806998056096,
      "loss": 4.6891,
      "step": 470
    },
    {
      "epoch": 0.13328705310656022,
      "grad_norm": 3.484375,
      "learning_rate": 0.0001866703693418495,
      "loss": 4.6914,
      "step": 480
    },
    {
      "epoch": 0.1360638667129469,
      "grad_norm": 3.71875,
      "learning_rate": 0.00018639266870313804,
      "loss": 4.5695,
      "step": 490
    },
    {
      "epoch": 0.13884068031933355,
      "grad_norm": 3.234375,
      "learning_rate": 0.00018611496806442655,
      "loss": 4.509,
      "step": 500
    },
    {
      "epoch": 0.14161749392572023,
      "grad_norm": 3.234375,
      "learning_rate": 0.00018583726742571507,
      "loss": 4.6531,
      "step": 510
    },
    {
      "epoch": 0.1443943075321069,
      "grad_norm": 3.109375,
      "learning_rate": 0.0001855595667870036,
      "loss": 4.459,
      "step": 520
    },
    {
      "epoch": 0.14717112113849357,
      "grad_norm": 3.71875,
      "learning_rate": 0.00018528186614829214,
      "loss": 4.768,
      "step": 530
    },
    {
      "epoch": 0.14994793474488025,
      "grad_norm": 3.265625,
      "learning_rate": 0.00018500416550958068,
      "loss": 4.4797,
      "step": 540
    },
    {
      "epoch": 0.15272474835126693,
      "grad_norm": 3.171875,
      "learning_rate": 0.00018472646487086922,
      "loss": 4.6445,
      "step": 550
    },
    {
      "epoch": 0.1555015619576536,
      "grad_norm": 3.5625,
      "learning_rate": 0.00018444876423215773,
      "loss": 4.4996,
      "step": 560
    },
    {
      "epoch": 0.15827837556404026,
      "grad_norm": 3.0625,
      "learning_rate": 0.00018417106359344627,
      "loss": 4.6254,
      "step": 570
    },
    {
      "epoch": 0.16105518917042694,
      "grad_norm": 3.484375,
      "learning_rate": 0.0001838933629547348,
      "loss": 4.6125,
      "step": 580
    },
    {
      "epoch": 0.16383200277681362,
      "grad_norm": 3.890625,
      "learning_rate": 0.00018361566231602335,
      "loss": 4.4742,
      "step": 590
    },
    {
      "epoch": 0.16660881638320027,
      "grad_norm": 3.3125,
      "learning_rate": 0.00018333796167731186,
      "loss": 4.4828,
      "step": 600
    },
    {
      "epoch": 0.16938562998958695,
      "grad_norm": 3.171875,
      "learning_rate": 0.0001830602610386004,
      "loss": 4.4777,
      "step": 610
    },
    {
      "epoch": 0.17216244359597363,
      "grad_norm": 3.53125,
      "learning_rate": 0.00018278256039988891,
      "loss": 4.5551,
      "step": 620
    },
    {
      "epoch": 0.17493925720236028,
      "grad_norm": 3.84375,
      "learning_rate": 0.00018250485976117745,
      "loss": 4.6305,
      "step": 630
    },
    {
      "epoch": 0.17771607080874696,
      "grad_norm": 3.953125,
      "learning_rate": 0.000182227159122466,
      "loss": 4.5617,
      "step": 640
    },
    {
      "epoch": 0.18049288441513364,
      "grad_norm": 3.53125,
      "learning_rate": 0.00018194945848375453,
      "loss": 4.748,
      "step": 650
    },
    {
      "epoch": 0.1832696980215203,
      "grad_norm": 3.390625,
      "learning_rate": 0.00018167175784504304,
      "loss": 4.6559,
      "step": 660
    },
    {
      "epoch": 0.18604651162790697,
      "grad_norm": 3.359375,
      "learning_rate": 0.00018139405720633158,
      "loss": 4.6832,
      "step": 670
    },
    {
      "epoch": 0.18882332523429365,
      "grad_norm": 3.40625,
      "learning_rate": 0.00018111635656762012,
      "loss": 4.3758,
      "step": 680
    },
    {
      "epoch": 0.1916001388406803,
      "grad_norm": 3.53125,
      "learning_rate": 0.00018083865592890866,
      "loss": 4.5105,
      "step": 690
    },
    {
      "epoch": 0.194376952447067,
      "grad_norm": 2.6875,
      "learning_rate": 0.0001805609552901972,
      "loss": 4.5238,
      "step": 700
    },
    {
      "epoch": 0.19715376605345367,
      "grad_norm": 3.734375,
      "learning_rate": 0.0001802832546514857,
      "loss": 4.5973,
      "step": 710
    },
    {
      "epoch": 0.19993057965984035,
      "grad_norm": 3.1875,
      "learning_rate": 0.00018000555401277422,
      "loss": 4.5016,
      "step": 720
    },
    {
      "epoch": 0.202707393266227,
      "grad_norm": 3.609375,
      "learning_rate": 0.00017972785337406276,
      "loss": 4.557,
      "step": 730
    },
    {
      "epoch": 0.20548420687261368,
      "grad_norm": 3.5,
      "learning_rate": 0.0001794501527353513,
      "loss": 4.4609,
      "step": 740
    },
    {
      "epoch": 0.20826102047900036,
      "grad_norm": 3.609375,
      "learning_rate": 0.0001791724520966398,
      "loss": 4.5379,
      "step": 750
    },
    {
      "epoch": 0.211037834085387,
      "grad_norm": 3.046875,
      "learning_rate": 0.00017889475145792835,
      "loss": 4.4707,
      "step": 760
    },
    {
      "epoch": 0.2138146476917737,
      "grad_norm": 3.8125,
      "learning_rate": 0.0001786170508192169,
      "loss": 4.6758,
      "step": 770
    },
    {
      "epoch": 0.21659146129816037,
      "grad_norm": 3.359375,
      "learning_rate": 0.00017833935018050543,
      "loss": 4.4273,
      "step": 780
    },
    {
      "epoch": 0.21936827490454702,
      "grad_norm": 3.15625,
      "learning_rate": 0.00017806164954179397,
      "loss": 4.4297,
      "step": 790
    },
    {
      "epoch": 0.2221450885109337,
      "grad_norm": 2.90625,
      "learning_rate": 0.00017778394890308248,
      "loss": 4.6051,
      "step": 800
    },
    {
      "epoch": 0.22492190211732038,
      "grad_norm": 3.734375,
      "learning_rate": 0.00017750624826437102,
      "loss": 4.6051,
      "step": 810
    },
    {
      "epoch": 0.22769871572370703,
      "grad_norm": 3.21875,
      "learning_rate": 0.00017722854762565956,
      "loss": 4.5105,
      "step": 820
    },
    {
      "epoch": 0.23047552933009371,
      "grad_norm": 2.859375,
      "learning_rate": 0.00017695084698694807,
      "loss": 4.7035,
      "step": 830
    },
    {
      "epoch": 0.2332523429364804,
      "grad_norm": 2.78125,
      "learning_rate": 0.0001766731463482366,
      "loss": 4.55,
      "step": 840
    },
    {
      "epoch": 0.23602915654286705,
      "grad_norm": 3.453125,
      "learning_rate": 0.00017639544570952512,
      "loss": 4.5535,
      "step": 850
    },
    {
      "epoch": 0.23880597014925373,
      "grad_norm": 3.46875,
      "learning_rate": 0.00017611774507081366,
      "loss": 4.5203,
      "step": 860
    },
    {
      "epoch": 0.2415827837556404,
      "grad_norm": 3.375,
      "learning_rate": 0.0001758400444321022,
      "loss": 4.5242,
      "step": 870
    },
    {
      "epoch": 0.2443595973620271,
      "grad_norm": 3.75,
      "learning_rate": 0.00017556234379339074,
      "loss": 4.5992,
      "step": 880
    },
    {
      "epoch": 0.24713641096841374,
      "grad_norm": 4.0,
      "learning_rate": 0.00017528464315467928,
      "loss": 4.3633,
      "step": 890
    },
    {
      "epoch": 0.24991322457480042,
      "grad_norm": 3.421875,
      "learning_rate": 0.0001750069425159678,
      "loss": 4.4637,
      "step": 900
    },
    {
      "epoch": 0.25269003818118707,
      "grad_norm": 3.46875,
      "learning_rate": 0.00017472924187725633,
      "loss": 4.6,
      "step": 910
    },
    {
      "epoch": 0.2554668517875738,
      "grad_norm": 3.71875,
      "learning_rate": 0.00017445154123854487,
      "loss": 4.5363,
      "step": 920
    },
    {
      "epoch": 0.25824366539396043,
      "grad_norm": 3.0,
      "learning_rate": 0.00017417384059983338,
      "loss": 4.5324,
      "step": 930
    },
    {
      "epoch": 0.2610204790003471,
      "grad_norm": 3.546875,
      "learning_rate": 0.00017389613996112192,
      "loss": 4.5566,
      "step": 940
    },
    {
      "epoch": 0.2637972926067338,
      "grad_norm": 4.3125,
      "learning_rate": 0.00017361843932241043,
      "loss": 4.4473,
      "step": 950
    },
    {
      "epoch": 0.26657410621312044,
      "grad_norm": 3.703125,
      "learning_rate": 0.00017334073868369897,
      "loss": 4.459,
      "step": 960
    },
    {
      "epoch": 0.2693509198195071,
      "grad_norm": 3.25,
      "learning_rate": 0.0001730630380449875,
      "loss": 4.4441,
      "step": 970
    },
    {
      "epoch": 0.2721277334258938,
      "grad_norm": 3.0,
      "learning_rate": 0.00017278533740627605,
      "loss": 4.4488,
      "step": 980
    },
    {
      "epoch": 0.27490454703228046,
      "grad_norm": 3.171875,
      "learning_rate": 0.0001725076367675646,
      "loss": 4.4453,
      "step": 990
    },
    {
      "epoch": 0.2776813606386671,
      "grad_norm": 4.125,
      "learning_rate": 0.0001722299361288531,
      "loss": 4.3301,
      "step": 1000
    },
    {
      "epoch": 0.2804581742450538,
      "grad_norm": 3.59375,
      "learning_rate": 0.00017195223549014164,
      "loss": 4.5805,
      "step": 1010
    },
    {
      "epoch": 0.28323498785144047,
      "grad_norm": 3.40625,
      "learning_rate": 0.00017167453485143018,
      "loss": 4.5359,
      "step": 1020
    },
    {
      "epoch": 0.2860118014578271,
      "grad_norm": 3.546875,
      "learning_rate": 0.00017139683421271872,
      "loss": 4.3965,
      "step": 1030
    },
    {
      "epoch": 0.2887886150642138,
      "grad_norm": 3.859375,
      "learning_rate": 0.00017111913357400723,
      "loss": 4.5016,
      "step": 1040
    },
    {
      "epoch": 0.2915654286706005,
      "grad_norm": 3.1875,
      "learning_rate": 0.00017084143293529574,
      "loss": 4.3359,
      "step": 1050
    },
    {
      "epoch": 0.29434224227698713,
      "grad_norm": 3.859375,
      "learning_rate": 0.00017056373229658428,
      "loss": 4.4176,
      "step": 1060
    },
    {
      "epoch": 0.29711905588337384,
      "grad_norm": 3.390625,
      "learning_rate": 0.00017028603165787282,
      "loss": 4.3316,
      "step": 1070
    },
    {
      "epoch": 0.2998958694897605,
      "grad_norm": 3.640625,
      "learning_rate": 0.00017000833101916136,
      "loss": 4.325,
      "step": 1080
    },
    {
      "epoch": 0.30267268309614714,
      "grad_norm": 3.25,
      "learning_rate": 0.00016973063038044987,
      "loss": 4.407,
      "step": 1090
    },
    {
      "epoch": 0.30544949670253385,
      "grad_norm": 2.78125,
      "learning_rate": 0.0001694529297417384,
      "loss": 4.475,
      "step": 1100
    },
    {
      "epoch": 0.3082263103089205,
      "grad_norm": 3.703125,
      "learning_rate": 0.00016917522910302695,
      "loss": 4.4844,
      "step": 1110
    },
    {
      "epoch": 0.3110031239153072,
      "grad_norm": 3.609375,
      "learning_rate": 0.00016889752846431549,
      "loss": 4.4207,
      "step": 1120
    },
    {
      "epoch": 0.31377993752169386,
      "grad_norm": 3.03125,
      "learning_rate": 0.00016861982782560403,
      "loss": 4.3402,
      "step": 1130
    },
    {
      "epoch": 0.3165567511280805,
      "grad_norm": 3.125,
      "learning_rate": 0.00016834212718689254,
      "loss": 4.402,
      "step": 1140
    },
    {
      "epoch": 0.3193335647344672,
      "grad_norm": 2.84375,
      "learning_rate": 0.00016806442654818108,
      "loss": 4.3863,
      "step": 1150
    },
    {
      "epoch": 0.3221103783408539,
      "grad_norm": 2.90625,
      "learning_rate": 0.0001677867259094696,
      "loss": 4.1789,
      "step": 1160
    },
    {
      "epoch": 0.32488719194724053,
      "grad_norm": 3.6875,
      "learning_rate": 0.00016750902527075813,
      "loss": 4.3941,
      "step": 1170
    },
    {
      "epoch": 0.32766400555362724,
      "grad_norm": 3.609375,
      "learning_rate": 0.00016723132463204667,
      "loss": 4.5563,
      "step": 1180
    },
    {
      "epoch": 0.3304408191600139,
      "grad_norm": 3.625,
      "learning_rate": 0.00016695362399333518,
      "loss": 4.3004,
      "step": 1190
    },
    {
      "epoch": 0.33321763276640054,
      "grad_norm": 3.6875,
      "learning_rate": 0.00016667592335462372,
      "loss": 4.4422,
      "step": 1200
    },
    {
      "epoch": 0.33599444637278725,
      "grad_norm": 3.171875,
      "learning_rate": 0.00016639822271591226,
      "loss": 4.3723,
      "step": 1210
    },
    {
      "epoch": 0.3387712599791739,
      "grad_norm": 3.890625,
      "learning_rate": 0.0001661205220772008,
      "loss": 4.3883,
      "step": 1220
    },
    {
      "epoch": 0.34154807358556055,
      "grad_norm": 3.0,
      "learning_rate": 0.00016584282143848933,
      "loss": 4.4559,
      "step": 1230
    },
    {
      "epoch": 0.34432488719194726,
      "grad_norm": 3.6875,
      "learning_rate": 0.00016556512079977785,
      "loss": 4.4473,
      "step": 1240
    },
    {
      "epoch": 0.3471017007983339,
      "grad_norm": 2.828125,
      "learning_rate": 0.00016528742016106639,
      "loss": 4.2746,
      "step": 1250
    },
    {
      "epoch": 0.34987851440472056,
      "grad_norm": 3.609375,
      "learning_rate": 0.00016500971952235492,
      "loss": 4.4633,
      "step": 1260
    },
    {
      "epoch": 0.35265532801110727,
      "grad_norm": 3.140625,
      "learning_rate": 0.00016473201888364344,
      "loss": 4.5184,
      "step": 1270
    },
    {
      "epoch": 0.3554321416174939,
      "grad_norm": 3.109375,
      "learning_rate": 0.00016445431824493198,
      "loss": 4.559,
      "step": 1280
    },
    {
      "epoch": 0.3582089552238806,
      "grad_norm": 3.203125,
      "learning_rate": 0.0001641766176062205,
      "loss": 4.4738,
      "step": 1290
    },
    {
      "epoch": 0.3609857688302673,
      "grad_norm": 3.203125,
      "learning_rate": 0.00016389891696750903,
      "loss": 4.407,
      "step": 1300
    },
    {
      "epoch": 0.36376258243665394,
      "grad_norm": 3.09375,
      "learning_rate": 0.00016362121632879757,
      "loss": 4.3371,
      "step": 1310
    },
    {
      "epoch": 0.3665393960430406,
      "grad_norm": 3.375,
      "learning_rate": 0.0001633435156900861,
      "loss": 4.4523,
      "step": 1320
    },
    {
      "epoch": 0.3693162096494273,
      "grad_norm": 3.3125,
      "learning_rate": 0.00016306581505137462,
      "loss": 4.3258,
      "step": 1330
    },
    {
      "epoch": 0.37209302325581395,
      "grad_norm": 3.3125,
      "learning_rate": 0.00016278811441266315,
      "loss": 4.3094,
      "step": 1340
    },
    {
      "epoch": 0.3748698368622006,
      "grad_norm": 3.078125,
      "learning_rate": 0.0001625104137739517,
      "loss": 4.3066,
      "step": 1350
    },
    {
      "epoch": 0.3776466504685873,
      "grad_norm": 3.109375,
      "learning_rate": 0.00016223271313524023,
      "loss": 4.341,
      "step": 1360
    },
    {
      "epoch": 0.38042346407497396,
      "grad_norm": 3.453125,
      "learning_rate": 0.00016195501249652874,
      "loss": 4.459,
      "step": 1370
    },
    {
      "epoch": 0.3832002776813606,
      "grad_norm": 3.25,
      "learning_rate": 0.00016167731185781728,
      "loss": 4.3992,
      "step": 1380
    },
    {
      "epoch": 0.3859770912877473,
      "grad_norm": 3.515625,
      "learning_rate": 0.0001613996112191058,
      "loss": 4.4387,
      "step": 1390
    },
    {
      "epoch": 0.388753904894134,
      "grad_norm": 3.015625,
      "learning_rate": 0.00016112191058039433,
      "loss": 4.4449,
      "step": 1400
    },
    {
      "epoch": 0.3915307185005207,
      "grad_norm": 3.1875,
      "learning_rate": 0.00016084420994168287,
      "loss": 4.5625,
      "step": 1410
    },
    {
      "epoch": 0.39430753210690733,
      "grad_norm": 3.140625,
      "learning_rate": 0.0001605665093029714,
      "loss": 4.5938,
      "step": 1420
    },
    {
      "epoch": 0.397084345713294,
      "grad_norm": 3.84375,
      "learning_rate": 0.00016028880866425992,
      "loss": 4.5012,
      "step": 1430
    },
    {
      "epoch": 0.3998611593196807,
      "grad_norm": 3.390625,
      "learning_rate": 0.00016001110802554846,
      "loss": 4.3809,
      "step": 1440
    },
    {
      "epoch": 0.40263797292606734,
      "grad_norm": 3.828125,
      "learning_rate": 0.000159733407386837,
      "loss": 4.3906,
      "step": 1450
    },
    {
      "epoch": 0.405414786532454,
      "grad_norm": 3.703125,
      "learning_rate": 0.00015945570674812554,
      "loss": 4.2457,
      "step": 1460
    },
    {
      "epoch": 0.4081916001388407,
      "grad_norm": 3.71875,
      "learning_rate": 0.00015917800610941408,
      "loss": 4.3754,
      "step": 1470
    },
    {
      "epoch": 0.41096841374522736,
      "grad_norm": 4.15625,
      "learning_rate": 0.0001589003054707026,
      "loss": 4.3238,
      "step": 1480
    },
    {
      "epoch": 0.413745227351614,
      "grad_norm": 3.65625,
      "learning_rate": 0.0001586226048319911,
      "loss": 4.2035,
      "step": 1490
    },
    {
      "epoch": 0.4165220409580007,
      "grad_norm": 3.0,
      "learning_rate": 0.00015834490419327964,
      "loss": 4.2902,
      "step": 1500
    },
    {
      "epoch": 0.41929885456438737,
      "grad_norm": 3.6875,
      "learning_rate": 0.00015806720355456818,
      "loss": 4.2043,
      "step": 1510
    },
    {
      "epoch": 0.422075668170774,
      "grad_norm": 3.875,
      "learning_rate": 0.00015778950291585672,
      "loss": 4.2719,
      "step": 1520
    },
    {
      "epoch": 0.42485248177716073,
      "grad_norm": 3.859375,
      "learning_rate": 0.00015751180227714523,
      "loss": 4.198,
      "step": 1530
    },
    {
      "epoch": 0.4276292953835474,
      "grad_norm": 3.578125,
      "learning_rate": 0.00015723410163843377,
      "loss": 4.3863,
      "step": 1540
    },
    {
      "epoch": 0.43040610898993403,
      "grad_norm": 3.25,
      "learning_rate": 0.0001569564009997223,
      "loss": 4.2605,
      "step": 1550
    },
    {
      "epoch": 0.43318292259632074,
      "grad_norm": 3.796875,
      "learning_rate": 0.00015667870036101085,
      "loss": 4.3508,
      "step": 1560
    },
    {
      "epoch": 0.4359597362027074,
      "grad_norm": 3.796875,
      "learning_rate": 0.0001564009997222994,
      "loss": 4.248,
      "step": 1570
    },
    {
      "epoch": 0.43873654980909405,
      "grad_norm": 3.5625,
      "learning_rate": 0.0001561232990835879,
      "loss": 4.4043,
      "step": 1580
    },
    {
      "epoch": 0.44151336341548075,
      "grad_norm": 3.109375,
      "learning_rate": 0.00015584559844487644,
      "loss": 4.3059,
      "step": 1590
    },
    {
      "epoch": 0.4442901770218674,
      "grad_norm": 3.4375,
      "learning_rate": 0.00015556789780616495,
      "loss": 4.3992,
      "step": 1600
    },
    {
      "epoch": 0.44706699062825406,
      "grad_norm": 3.84375,
      "learning_rate": 0.0001552901971674535,
      "loss": 4.2703,
      "step": 1610
    },
    {
      "epoch": 0.44984380423464077,
      "grad_norm": 3.875,
      "learning_rate": 0.000155012496528742,
      "loss": 4.2484,
      "step": 1620
    },
    {
      "epoch": 0.4526206178410274,
      "grad_norm": 7.21875,
      "learning_rate": 0.00015473479589003054,
      "loss": 4.1426,
      "step": 1630
    },
    {
      "epoch": 0.45539743144741407,
      "grad_norm": 4.21875,
      "learning_rate": 0.00015445709525131908,
      "loss": 4.3738,
      "step": 1640
    },
    {
      "epoch": 0.4581742450538008,
      "grad_norm": 3.265625,
      "learning_rate": 0.00015417939461260762,
      "loss": 4.4,
      "step": 1650
    },
    {
      "epoch": 0.46095105866018743,
      "grad_norm": 3.78125,
      "learning_rate": 0.00015390169397389616,
      "loss": 4.2914,
      "step": 1660
    },
    {
      "epoch": 0.4637278722665741,
      "grad_norm": 2.78125,
      "learning_rate": 0.00015362399333518467,
      "loss": 4.4293,
      "step": 1670
    },
    {
      "epoch": 0.4665046858729608,
      "grad_norm": 3.828125,
      "learning_rate": 0.0001533462926964732,
      "loss": 4.2727,
      "step": 1680
    },
    {
      "epoch": 0.46928149947934744,
      "grad_norm": 3.859375,
      "learning_rate": 0.00015306859205776175,
      "loss": 4.4176,
      "step": 1690
    },
    {
      "epoch": 0.4720583130857341,
      "grad_norm": 3.328125,
      "learning_rate": 0.00015279089141905026,
      "loss": 4.3,
      "step": 1700
    },
    {
      "epoch": 0.4748351266921208,
      "grad_norm": 3.671875,
      "learning_rate": 0.0001525131907803388,
      "loss": 4.3563,
      "step": 1710
    },
    {
      "epoch": 0.47761194029850745,
      "grad_norm": 3.90625,
      "learning_rate": 0.0001522354901416273,
      "loss": 4.3664,
      "step": 1720
    },
    {
      "epoch": 0.48038875390489416,
      "grad_norm": 4.15625,
      "learning_rate": 0.00015195778950291585,
      "loss": 4.3406,
      "step": 1730
    },
    {
      "epoch": 0.4831655675112808,
      "grad_norm": 3.75,
      "learning_rate": 0.0001516800888642044,
      "loss": 4.3445,
      "step": 1740
    },
    {
      "epoch": 0.48594238111766747,
      "grad_norm": 5.78125,
      "learning_rate": 0.00015140238822549293,
      "loss": 4.2754,
      "step": 1750
    },
    {
      "epoch": 0.4887191947240542,
      "grad_norm": 4.28125,
      "learning_rate": 0.00015112468758678147,
      "loss": 4.1078,
      "step": 1760
    },
    {
      "epoch": 0.4914960083304408,
      "grad_norm": 10.625,
      "learning_rate": 0.00015084698694806998,
      "loss": 4.2863,
      "step": 1770
    },
    {
      "epoch": 0.4942728219368275,
      "grad_norm": 3.890625,
      "learning_rate": 0.00015056928630935852,
      "loss": 4.1711,
      "step": 1780
    },
    {
      "epoch": 0.4970496355432142,
      "grad_norm": 3.578125,
      "learning_rate": 0.00015029158567064706,
      "loss": 4.4227,
      "step": 1790
    },
    {
      "epoch": 0.49982644914960084,
      "grad_norm": 3.9375,
      "learning_rate": 0.0001500138850319356,
      "loss": 4.3582,
      "step": 1800
    },
    {
      "epoch": 0.5026032627559875,
      "grad_norm": 3.59375,
      "learning_rate": 0.0001497361843932241,
      "loss": 4.3641,
      "step": 1810
    },
    {
      "epoch": 0.5053800763623741,
      "grad_norm": 3.5,
      "learning_rate": 0.00014945848375451262,
      "loss": 4.2551,
      "step": 1820
    },
    {
      "epoch": 0.5081568899687608,
      "grad_norm": 3.890625,
      "learning_rate": 0.00014918078311580116,
      "loss": 4.3199,
      "step": 1830
    },
    {
      "epoch": 0.5109337035751476,
      "grad_norm": 3.296875,
      "learning_rate": 0.0001489030824770897,
      "loss": 4.3309,
      "step": 1840
    },
    {
      "epoch": 0.5137105171815342,
      "grad_norm": 3.59375,
      "learning_rate": 0.00014862538183837824,
      "loss": 4.5035,
      "step": 1850
    },
    {
      "epoch": 0.5164873307879209,
      "grad_norm": 4.125,
      "learning_rate": 0.00014834768119966678,
      "loss": 4.4121,
      "step": 1860
    },
    {
      "epoch": 0.5192641443943076,
      "grad_norm": 3.515625,
      "learning_rate": 0.0001480699805609553,
      "loss": 4.3676,
      "step": 1870
    },
    {
      "epoch": 0.5220409580006942,
      "grad_norm": 3.6875,
      "learning_rate": 0.00014779227992224383,
      "loss": 4.3,
      "step": 1880
    },
    {
      "epoch": 0.5248177716070809,
      "grad_norm": 3.28125,
      "learning_rate": 0.00014751457928353237,
      "loss": 4.2629,
      "step": 1890
    },
    {
      "epoch": 0.5275945852134676,
      "grad_norm": 3.6875,
      "learning_rate": 0.0001472368786448209,
      "loss": 4.2633,
      "step": 1900
    },
    {
      "epoch": 0.5303713988198542,
      "grad_norm": 3.71875,
      "learning_rate": 0.00014695917800610942,
      "loss": 4.1918,
      "step": 1910
    },
    {
      "epoch": 0.5331482124262409,
      "grad_norm": 4.125,
      "learning_rate": 0.00014668147736739796,
      "loss": 4.2934,
      "step": 1920
    },
    {
      "epoch": 0.5359250260326276,
      "grad_norm": 3.6875,
      "learning_rate": 0.00014640377672868647,
      "loss": 4.2703,
      "step": 1930
    },
    {
      "epoch": 0.5387018396390142,
      "grad_norm": 3.6875,
      "learning_rate": 0.000146126076089975,
      "loss": 4.2773,
      "step": 1940
    },
    {
      "epoch": 0.5414786532454009,
      "grad_norm": 3.703125,
      "learning_rate": 0.00014584837545126355,
      "loss": 4.2207,
      "step": 1950
    },
    {
      "epoch": 0.5442554668517876,
      "grad_norm": 4.71875,
      "learning_rate": 0.00014557067481255206,
      "loss": 4.2715,
      "step": 1960
    },
    {
      "epoch": 0.5470322804581742,
      "grad_norm": 3.640625,
      "learning_rate": 0.0001452929741738406,
      "loss": 4.4672,
      "step": 1970
    },
    {
      "epoch": 0.5498090940645609,
      "grad_norm": 3.828125,
      "learning_rate": 0.00014501527353512914,
      "loss": 4.3438,
      "step": 1980
    },
    {
      "epoch": 0.5525859076709476,
      "grad_norm": 4.375,
      "learning_rate": 0.00014473757289641768,
      "loss": 4.3402,
      "step": 1990
    },
    {
      "epoch": 0.5553627212773342,
      "grad_norm": 5.40625,
      "learning_rate": 0.00014445987225770622,
      "loss": 4.2355,
      "step": 2000
    },
    {
      "epoch": 0.5581395348837209,
      "grad_norm": 3.75,
      "learning_rate": 0.00014418217161899473,
      "loss": 4.3812,
      "step": 2010
    },
    {
      "epoch": 0.5609163484901076,
      "grad_norm": 5.59375,
      "learning_rate": 0.00014390447098028327,
      "loss": 4.4254,
      "step": 2020
    },
    {
      "epoch": 0.5636931620964942,
      "grad_norm": 4.4375,
      "learning_rate": 0.0001436267703415718,
      "loss": 4.2551,
      "step": 2030
    },
    {
      "epoch": 0.5664699757028809,
      "grad_norm": 3.921875,
      "learning_rate": 0.00014334906970286032,
      "loss": 4.3316,
      "step": 2040
    },
    {
      "epoch": 0.5692467893092676,
      "grad_norm": 2.953125,
      "learning_rate": 0.00014307136906414886,
      "loss": 4.2402,
      "step": 2050
    },
    {
      "epoch": 0.5720236029156542,
      "grad_norm": 3.828125,
      "learning_rate": 0.00014279366842543737,
      "loss": 4.357,
      "step": 2060
    },
    {
      "epoch": 0.574800416522041,
      "grad_norm": 3.28125,
      "learning_rate": 0.0001425159677867259,
      "loss": 4.3473,
      "step": 2070
    },
    {
      "epoch": 0.5775772301284277,
      "grad_norm": 3.265625,
      "learning_rate": 0.00014223826714801445,
      "loss": 4.2863,
      "step": 2080
    },
    {
      "epoch": 0.5803540437348143,
      "grad_norm": 3.921875,
      "learning_rate": 0.00014196056650930299,
      "loss": 4.2965,
      "step": 2090
    },
    {
      "epoch": 0.583130857341201,
      "grad_norm": 3.671875,
      "learning_rate": 0.00014168286587059152,
      "loss": 4.2652,
      "step": 2100
    },
    {
      "epoch": 0.5859076709475877,
      "grad_norm": 9.3125,
      "learning_rate": 0.00014140516523188004,
      "loss": 4.3281,
      "step": 2110
    },
    {
      "epoch": 0.5886844845539743,
      "grad_norm": 3.796875,
      "learning_rate": 0.00014112746459316858,
      "loss": 4.3844,
      "step": 2120
    },
    {
      "epoch": 0.591461298160361,
      "grad_norm": 3.875,
      "learning_rate": 0.00014084976395445711,
      "loss": 4.0801,
      "step": 2130
    },
    {
      "epoch": 0.5942381117667477,
      "grad_norm": 3.328125,
      "learning_rate": 0.00014057206331574563,
      "loss": 4.3875,
      "step": 2140
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 3.828125,
      "learning_rate": 0.00014029436267703417,
      "loss": 4.3016,
      "step": 2150
    },
    {
      "epoch": 0.599791738979521,
      "grad_norm": 4.0625,
      "learning_rate": 0.00014001666203832268,
      "loss": 4.1781,
      "step": 2160
    },
    {
      "epoch": 0.6025685525859077,
      "grad_norm": 3.296875,
      "learning_rate": 0.00013973896139961122,
      "loss": 4.1469,
      "step": 2170
    },
    {
      "epoch": 0.6053453661922943,
      "grad_norm": 3.703125,
      "learning_rate": 0.00013946126076089976,
      "loss": 4.2242,
      "step": 2180
    },
    {
      "epoch": 0.608122179798681,
      "grad_norm": 3.46875,
      "learning_rate": 0.0001391835601221883,
      "loss": 4.2648,
      "step": 2190
    },
    {
      "epoch": 0.6108989934050677,
      "grad_norm": 3.796875,
      "learning_rate": 0.0001389058594834768,
      "loss": 4.1859,
      "step": 2200
    },
    {
      "epoch": 0.6136758070114544,
      "grad_norm": 3.421875,
      "learning_rate": 0.00013862815884476535,
      "loss": 4.1324,
      "step": 2210
    },
    {
      "epoch": 0.616452620617841,
      "grad_norm": 4.28125,
      "learning_rate": 0.00013835045820605388,
      "loss": 4.1273,
      "step": 2220
    },
    {
      "epoch": 0.6192294342242277,
      "grad_norm": 3.515625,
      "learning_rate": 0.00013807275756734242,
      "loss": 4.3969,
      "step": 2230
    },
    {
      "epoch": 0.6220062478306144,
      "grad_norm": 5.4375,
      "learning_rate": 0.00013779505692863096,
      "loss": 4.5422,
      "step": 2240
    },
    {
      "epoch": 0.624783061437001,
      "grad_norm": 3.890625,
      "learning_rate": 0.00013751735628991947,
      "loss": 4.3402,
      "step": 2250
    },
    {
      "epoch": 0.6275598750433877,
      "grad_norm": 5.34375,
      "learning_rate": 0.000137239655651208,
      "loss": 4.3152,
      "step": 2260
    },
    {
      "epoch": 0.6303366886497744,
      "grad_norm": 3.578125,
      "learning_rate": 0.00013696195501249653,
      "loss": 4.2789,
      "step": 2270
    },
    {
      "epoch": 0.633113502256161,
      "grad_norm": 3.234375,
      "learning_rate": 0.00013668425437378506,
      "loss": 4.1676,
      "step": 2280
    },
    {
      "epoch": 0.6358903158625477,
      "grad_norm": 3.8125,
      "learning_rate": 0.0001364065537350736,
      "loss": 4.3187,
      "step": 2290
    },
    {
      "epoch": 0.6386671294689344,
      "grad_norm": 3.421875,
      "learning_rate": 0.00013612885309636212,
      "loss": 4.0898,
      "step": 2300
    },
    {
      "epoch": 0.641443943075321,
      "grad_norm": 4.5,
      "learning_rate": 0.00013585115245765065,
      "loss": 4.3465,
      "step": 2310
    },
    {
      "epoch": 0.6442207566817078,
      "grad_norm": 3.703125,
      "learning_rate": 0.0001355734518189392,
      "loss": 3.9691,
      "step": 2320
    },
    {
      "epoch": 0.6469975702880945,
      "grad_norm": 3.359375,
      "learning_rate": 0.00013529575118022773,
      "loss": 4.2781,
      "step": 2330
    },
    {
      "epoch": 0.6497743838944811,
      "grad_norm": 3.625,
      "learning_rate": 0.00013501805054151627,
      "loss": 4.4039,
      "step": 2340
    },
    {
      "epoch": 0.6525511975008678,
      "grad_norm": 4.46875,
      "learning_rate": 0.00013474034990280478,
      "loss": 4.3406,
      "step": 2350
    },
    {
      "epoch": 0.6553280111072545,
      "grad_norm": 4.34375,
      "learning_rate": 0.00013446264926409332,
      "loss": 4.1691,
      "step": 2360
    },
    {
      "epoch": 0.6581048247136411,
      "grad_norm": 3.359375,
      "learning_rate": 0.00013418494862538183,
      "loss": 4.316,
      "step": 2370
    },
    {
      "epoch": 0.6608816383200278,
      "grad_norm": 3.796875,
      "learning_rate": 0.00013390724798667037,
      "loss": 4.2195,
      "step": 2380
    },
    {
      "epoch": 0.6636584519264145,
      "grad_norm": 3.796875,
      "learning_rate": 0.0001336295473479589,
      "loss": 4.2809,
      "step": 2390
    },
    {
      "epoch": 0.6664352655328011,
      "grad_norm": 6.75,
      "learning_rate": 0.00013335184670924742,
      "loss": 4.6027,
      "step": 2400
    },
    {
      "epoch": 0.6692120791391878,
      "grad_norm": 3.703125,
      "learning_rate": 0.00013307414607053596,
      "loss": 4.4398,
      "step": 2410
    },
    {
      "epoch": 0.6719888927455745,
      "grad_norm": 3.5,
      "learning_rate": 0.0001327964454318245,
      "loss": 4.1887,
      "step": 2420
    },
    {
      "epoch": 0.6747657063519611,
      "grad_norm": 3.578125,
      "learning_rate": 0.00013251874479311304,
      "loss": 4.1645,
      "step": 2430
    },
    {
      "epoch": 0.6775425199583478,
      "grad_norm": 3.484375,
      "learning_rate": 0.00013224104415440158,
      "loss": 4.273,
      "step": 2440
    },
    {
      "epoch": 0.6803193335647345,
      "grad_norm": 3.578125,
      "learning_rate": 0.0001319633435156901,
      "loss": 4.35,
      "step": 2450
    },
    {
      "epoch": 0.6830961471711211,
      "grad_norm": 3.40625,
      "learning_rate": 0.00013168564287697863,
      "loss": 4.2445,
      "step": 2460
    },
    {
      "epoch": 0.6858729607775078,
      "grad_norm": 3.8125,
      "learning_rate": 0.00013140794223826714,
      "loss": 4.2625,
      "step": 2470
    },
    {
      "epoch": 0.6886497743838945,
      "grad_norm": 3.578125,
      "learning_rate": 0.00013113024159955568,
      "loss": 4.2727,
      "step": 2480
    },
    {
      "epoch": 0.6914265879902811,
      "grad_norm": 3.328125,
      "learning_rate": 0.0001308525409608442,
      "loss": 4.1199,
      "step": 2490
    },
    {
      "epoch": 0.6942034015966678,
      "grad_norm": 3.421875,
      "learning_rate": 0.00013057484032213273,
      "loss": 4.277,
      "step": 2500
    },
    {
      "epoch": 0.6969802152030545,
      "grad_norm": 3.515625,
      "learning_rate": 0.00013029713968342127,
      "loss": 4.1656,
      "step": 2510
    },
    {
      "epoch": 0.6997570288094411,
      "grad_norm": 3.53125,
      "learning_rate": 0.0001300194390447098,
      "loss": 4.202,
      "step": 2520
    },
    {
      "epoch": 0.7025338424158278,
      "grad_norm": 4.25,
      "learning_rate": 0.00012974173840599835,
      "loss": 4.202,
      "step": 2530
    },
    {
      "epoch": 0.7053106560222145,
      "grad_norm": 3.9375,
      "learning_rate": 0.00012946403776728686,
      "loss": 4.4148,
      "step": 2540
    },
    {
      "epoch": 0.7080874696286011,
      "grad_norm": 3.578125,
      "learning_rate": 0.0001291863371285754,
      "loss": 4.0961,
      "step": 2550
    },
    {
      "epoch": 0.7108642832349878,
      "grad_norm": 3.3125,
      "learning_rate": 0.00012890863648986394,
      "loss": 4.2566,
      "step": 2560
    },
    {
      "epoch": 0.7136410968413746,
      "grad_norm": 4.15625,
      "learning_rate": 0.00012863093585115248,
      "loss": 4.273,
      "step": 2570
    },
    {
      "epoch": 0.7164179104477612,
      "grad_norm": 3.8125,
      "learning_rate": 0.000128353235212441,
      "loss": 4.357,
      "step": 2580
    },
    {
      "epoch": 0.7191947240541479,
      "grad_norm": 3.890625,
      "learning_rate": 0.0001280755345737295,
      "loss": 4.1559,
      "step": 2590
    },
    {
      "epoch": 0.7219715376605346,
      "grad_norm": 3.0625,
      "learning_rate": 0.00012779783393501804,
      "loss": 4.357,
      "step": 2600
    },
    {
      "epoch": 0.7247483512669212,
      "grad_norm": 4.09375,
      "learning_rate": 0.00012752013329630658,
      "loss": 4.1293,
      "step": 2610
    },
    {
      "epoch": 0.7275251648733079,
      "grad_norm": 3.3125,
      "learning_rate": 0.00012724243265759512,
      "loss": 4.3684,
      "step": 2620
    },
    {
      "epoch": 0.7303019784796946,
      "grad_norm": 3.5,
      "learning_rate": 0.00012696473201888366,
      "loss": 4.2891,
      "step": 2630
    },
    {
      "epoch": 0.7330787920860812,
      "grad_norm": 3.9375,
      "learning_rate": 0.00012668703138017217,
      "loss": 4.3086,
      "step": 2640
    },
    {
      "epoch": 0.7358556056924679,
      "grad_norm": 3.609375,
      "learning_rate": 0.0001264093307414607,
      "loss": 4.0852,
      "step": 2650
    },
    {
      "epoch": 0.7386324192988546,
      "grad_norm": 3.859375,
      "learning_rate": 0.00012613163010274925,
      "loss": 4.0578,
      "step": 2660
    },
    {
      "epoch": 0.7414092329052412,
      "grad_norm": 4.15625,
      "learning_rate": 0.0001258539294640378,
      "loss": 4.1176,
      "step": 2670
    },
    {
      "epoch": 0.7441860465116279,
      "grad_norm": 3.609375,
      "learning_rate": 0.00012557622882532633,
      "loss": 4.3297,
      "step": 2680
    },
    {
      "epoch": 0.7469628601180146,
      "grad_norm": 4.625,
      "learning_rate": 0.00012529852818661484,
      "loss": 4.0941,
      "step": 2690
    },
    {
      "epoch": 0.7497396737244012,
      "grad_norm": 4.75,
      "learning_rate": 0.00012502082754790335,
      "loss": 4.2828,
      "step": 2700
    },
    {
      "epoch": 0.7525164873307879,
      "grad_norm": 4.65625,
      "learning_rate": 0.0001247431269091919,
      "loss": 4.1613,
      "step": 2710
    },
    {
      "epoch": 0.7552933009371746,
      "grad_norm": 4.0625,
      "learning_rate": 0.00012446542627048043,
      "loss": 4.2445,
      "step": 2720
    },
    {
      "epoch": 0.7580701145435612,
      "grad_norm": 3.375,
      "learning_rate": 0.00012418772563176897,
      "loss": 4.1664,
      "step": 2730
    },
    {
      "epoch": 0.7608469281499479,
      "grad_norm": 3.859375,
      "learning_rate": 0.00012391002499305748,
      "loss": 4.1262,
      "step": 2740
    },
    {
      "epoch": 0.7636237417563346,
      "grad_norm": 3.484375,
      "learning_rate": 0.00012363232435434602,
      "loss": 4.0676,
      "step": 2750
    },
    {
      "epoch": 0.7664005553627212,
      "grad_norm": 3.6875,
      "learning_rate": 0.00012335462371563456,
      "loss": 4.3164,
      "step": 2760
    },
    {
      "epoch": 0.7691773689691079,
      "grad_norm": 4.40625,
      "learning_rate": 0.0001230769230769231,
      "loss": 4.0867,
      "step": 2770
    },
    {
      "epoch": 0.7719541825754946,
      "grad_norm": 4.65625,
      "learning_rate": 0.0001227992224382116,
      "loss": 4.2637,
      "step": 2780
    },
    {
      "epoch": 0.7747309961818812,
      "grad_norm": 5.125,
      "learning_rate": 0.00012252152179950015,
      "loss": 4.2895,
      "step": 2790
    },
    {
      "epoch": 0.777507809788268,
      "grad_norm": 3.546875,
      "learning_rate": 0.0001222438211607887,
      "loss": 4.2297,
      "step": 2800
    },
    {
      "epoch": 0.7802846233946547,
      "grad_norm": 3.90625,
      "learning_rate": 0.00012196612052207721,
      "loss": 3.9945,
      "step": 2810
    },
    {
      "epoch": 0.7830614370010414,
      "grad_norm": 3.765625,
      "learning_rate": 0.00012168841988336575,
      "loss": 4.1004,
      "step": 2820
    },
    {
      "epoch": 0.785838250607428,
      "grad_norm": 3.328125,
      "learning_rate": 0.00012141071924465426,
      "loss": 4.25,
      "step": 2830
    },
    {
      "epoch": 0.7886150642138147,
      "grad_norm": 3.046875,
      "learning_rate": 0.00012113301860594279,
      "loss": 4.3555,
      "step": 2840
    },
    {
      "epoch": 0.7913918778202014,
      "grad_norm": 3.875,
      "learning_rate": 0.00012085531796723133,
      "loss": 4.1836,
      "step": 2850
    },
    {
      "epoch": 0.794168691426588,
      "grad_norm": 3.953125,
      "learning_rate": 0.00012057761732851987,
      "loss": 4.0742,
      "step": 2860
    },
    {
      "epoch": 0.7969455050329747,
      "grad_norm": 3.8125,
      "learning_rate": 0.0001202999166898084,
      "loss": 4.2188,
      "step": 2870
    },
    {
      "epoch": 0.7997223186393614,
      "grad_norm": 4.34375,
      "learning_rate": 0.00012002221605109692,
      "loss": 4.2777,
      "step": 2880
    },
    {
      "epoch": 0.802499132245748,
      "grad_norm": 3.4375,
      "learning_rate": 0.00011974451541238544,
      "loss": 4.1926,
      "step": 2890
    },
    {
      "epoch": 0.8052759458521347,
      "grad_norm": 4.0625,
      "learning_rate": 0.00011946681477367398,
      "loss": 3.9914,
      "step": 2900
    },
    {
      "epoch": 0.8080527594585214,
      "grad_norm": 3.65625,
      "learning_rate": 0.00011918911413496252,
      "loss": 4.193,
      "step": 2910
    },
    {
      "epoch": 0.810829573064908,
      "grad_norm": 3.5,
      "learning_rate": 0.00011891141349625106,
      "loss": 4.0734,
      "step": 2920
    },
    {
      "epoch": 0.8136063866712947,
      "grad_norm": 4.34375,
      "learning_rate": 0.00011863371285753957,
      "loss": 4.2773,
      "step": 2930
    },
    {
      "epoch": 0.8163832002776814,
      "grad_norm": 3.546875,
      "learning_rate": 0.00011835601221882811,
      "loss": 4.2262,
      "step": 2940
    },
    {
      "epoch": 0.819160013884068,
      "grad_norm": 3.328125,
      "learning_rate": 0.00011807831158011664,
      "loss": 4.1598,
      "step": 2950
    },
    {
      "epoch": 0.8219368274904547,
      "grad_norm": 3.765625,
      "learning_rate": 0.00011780061094140518,
      "loss": 4.0418,
      "step": 2960
    },
    {
      "epoch": 0.8247136410968414,
      "grad_norm": 3.234375,
      "learning_rate": 0.00011752291030269372,
      "loss": 4.2699,
      "step": 2970
    },
    {
      "epoch": 0.827490454703228,
      "grad_norm": 4.15625,
      "learning_rate": 0.00011724520966398223,
      "loss": 4.0863,
      "step": 2980
    },
    {
      "epoch": 0.8302672683096147,
      "grad_norm": 4.28125,
      "learning_rate": 0.00011696750902527077,
      "loss": 4.3199,
      "step": 2990
    },
    {
      "epoch": 0.8330440819160014,
      "grad_norm": 3.5,
      "learning_rate": 0.00011668980838655929,
      "loss": 4.3133,
      "step": 3000
    },
    {
      "epoch": 0.835820895522388,
      "grad_norm": 4.21875,
      "learning_rate": 0.00011641210774784783,
      "loss": 4.4887,
      "step": 3010
    },
    {
      "epoch": 0.8385977091287747,
      "grad_norm": 3.671875,
      "learning_rate": 0.00011613440710913637,
      "loss": 4.175,
      "step": 3020
    },
    {
      "epoch": 0.8413745227351614,
      "grad_norm": 3.953125,
      "learning_rate": 0.00011585670647042488,
      "loss": 4.1336,
      "step": 3030
    },
    {
      "epoch": 0.844151336341548,
      "grad_norm": 4.28125,
      "learning_rate": 0.00011557900583171342,
      "loss": 4.3273,
      "step": 3040
    },
    {
      "epoch": 0.8469281499479347,
      "grad_norm": 4.9375,
      "learning_rate": 0.00011530130519300195,
      "loss": 4.343,
      "step": 3050
    },
    {
      "epoch": 0.8497049635543215,
      "grad_norm": 3.75,
      "learning_rate": 0.00011502360455429049,
      "loss": 4.2062,
      "step": 3060
    },
    {
      "epoch": 0.852481777160708,
      "grad_norm": 3.84375,
      "learning_rate": 0.000114745903915579,
      "loss": 4.0961,
      "step": 3070
    },
    {
      "epoch": 0.8552585907670948,
      "grad_norm": 4.40625,
      "learning_rate": 0.00011446820327686754,
      "loss": 4.2059,
      "step": 3080
    },
    {
      "epoch": 0.8580354043734815,
      "grad_norm": 3.546875,
      "learning_rate": 0.00011419050263815608,
      "loss": 4.2898,
      "step": 3090
    },
    {
      "epoch": 0.8608122179798681,
      "grad_norm": 4.03125,
      "learning_rate": 0.0001139128019994446,
      "loss": 4.3551,
      "step": 3100
    },
    {
      "epoch": 0.8635890315862548,
      "grad_norm": 4.1875,
      "learning_rate": 0.00011363510136073314,
      "loss": 4.1406,
      "step": 3110
    },
    {
      "epoch": 0.8663658451926415,
      "grad_norm": 3.453125,
      "learning_rate": 0.00011335740072202165,
      "loss": 4.1137,
      "step": 3120
    },
    {
      "epoch": 0.8691426587990281,
      "grad_norm": 4.625,
      "learning_rate": 0.00011307970008331019,
      "loss": 4.3797,
      "step": 3130
    },
    {
      "epoch": 0.8719194724054148,
      "grad_norm": 4.1875,
      "learning_rate": 0.00011280199944459873,
      "loss": 4.2445,
      "step": 3140
    },
    {
      "epoch": 0.8746962860118015,
      "grad_norm": 3.96875,
      "learning_rate": 0.00011252429880588727,
      "loss": 4.2328,
      "step": 3150
    },
    {
      "epoch": 0.8774730996181881,
      "grad_norm": 4.0625,
      "learning_rate": 0.0001122465981671758,
      "loss": 4.2578,
      "step": 3160
    },
    {
      "epoch": 0.8802499132245748,
      "grad_norm": 3.8125,
      "learning_rate": 0.0001119688975284643,
      "loss": 4.1828,
      "step": 3170
    },
    {
      "epoch": 0.8830267268309615,
      "grad_norm": 3.390625,
      "learning_rate": 0.00011169119688975285,
      "loss": 4.1387,
      "step": 3180
    },
    {
      "epoch": 0.8858035404373481,
      "grad_norm": 5.0,
      "learning_rate": 0.00011141349625104138,
      "loss": 4.3547,
      "step": 3190
    },
    {
      "epoch": 0.8885803540437348,
      "grad_norm": 3.484375,
      "learning_rate": 0.00011113579561232992,
      "loss": 4.1957,
      "step": 3200
    },
    {
      "epoch": 0.8913571676501215,
      "grad_norm": 3.8125,
      "learning_rate": 0.00011085809497361845,
      "loss": 4.277,
      "step": 3210
    },
    {
      "epoch": 0.8941339812565081,
      "grad_norm": 3.890625,
      "learning_rate": 0.00011058039433490696,
      "loss": 4.3828,
      "step": 3220
    },
    {
      "epoch": 0.8969107948628948,
      "grad_norm": 4.125,
      "learning_rate": 0.0001103026936961955,
      "loss": 4.0941,
      "step": 3230
    },
    {
      "epoch": 0.8996876084692815,
      "grad_norm": 3.09375,
      "learning_rate": 0.00011002499305748404,
      "loss": 4.2594,
      "step": 3240
    },
    {
      "epoch": 0.9024644220756681,
      "grad_norm": 3.5625,
      "learning_rate": 0.00010974729241877258,
      "loss": 4.1957,
      "step": 3250
    },
    {
      "epoch": 0.9052412356820548,
      "grad_norm": 3.546875,
      "learning_rate": 0.0001094695917800611,
      "loss": 4.0395,
      "step": 3260
    },
    {
      "epoch": 0.9080180492884415,
      "grad_norm": 3.578125,
      "learning_rate": 0.00010919189114134963,
      "loss": 4.1375,
      "step": 3270
    },
    {
      "epoch": 0.9107948628948281,
      "grad_norm": 3.96875,
      "learning_rate": 0.00010891419050263815,
      "loss": 3.9758,
      "step": 3280
    },
    {
      "epoch": 0.9135716765012148,
      "grad_norm": 4.59375,
      "learning_rate": 0.00010863648986392669,
      "loss": 4.1902,
      "step": 3290
    },
    {
      "epoch": 0.9163484901076016,
      "grad_norm": 3.90625,
      "learning_rate": 0.00010835878922521523,
      "loss": 4.232,
      "step": 3300
    },
    {
      "epoch": 0.9191253037139882,
      "grad_norm": 3.515625,
      "learning_rate": 0.00010808108858650377,
      "loss": 4.1465,
      "step": 3310
    },
    {
      "epoch": 0.9219021173203749,
      "grad_norm": 4.65625,
      "learning_rate": 0.00010780338794779228,
      "loss": 4.4336,
      "step": 3320
    },
    {
      "epoch": 0.9246789309267616,
      "grad_norm": 3.984375,
      "learning_rate": 0.00010752568730908081,
      "loss": 4.4668,
      "step": 3330
    },
    {
      "epoch": 0.9274557445331482,
      "grad_norm": 3.703125,
      "learning_rate": 0.00010724798667036935,
      "loss": 4.0852,
      "step": 3340
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 4.90625,
      "learning_rate": 0.00010697028603165789,
      "loss": 4.0836,
      "step": 3350
    },
    {
      "epoch": 0.9330093717459216,
      "grad_norm": 4.84375,
      "learning_rate": 0.0001066925853929464,
      "loss": 4.2297,
      "step": 3360
    },
    {
      "epoch": 0.9357861853523082,
      "grad_norm": 7.21875,
      "learning_rate": 0.00010641488475423494,
      "loss": 4.1855,
      "step": 3370
    },
    {
      "epoch": 0.9385629989586949,
      "grad_norm": 3.578125,
      "learning_rate": 0.00010613718411552346,
      "loss": 4.2207,
      "step": 3380
    },
    {
      "epoch": 0.9413398125650816,
      "grad_norm": 3.75,
      "learning_rate": 0.000105859483476812,
      "loss": 4.3906,
      "step": 3390
    },
    {
      "epoch": 0.9441166261714682,
      "grad_norm": 3.75,
      "learning_rate": 0.00010558178283810054,
      "loss": 4.2613,
      "step": 3400
    },
    {
      "epoch": 0.9468934397778549,
      "grad_norm": 4.65625,
      "learning_rate": 0.00010530408219938905,
      "loss": 4.2547,
      "step": 3410
    },
    {
      "epoch": 0.9496702533842416,
      "grad_norm": 4.25,
      "learning_rate": 0.00010502638156067759,
      "loss": 4.1965,
      "step": 3420
    },
    {
      "epoch": 0.9524470669906282,
      "grad_norm": 4.9375,
      "learning_rate": 0.00010474868092196613,
      "loss": 4.0926,
      "step": 3430
    },
    {
      "epoch": 0.9552238805970149,
      "grad_norm": 3.3125,
      "learning_rate": 0.00010447098028325466,
      "loss": 4.1152,
      "step": 3440
    },
    {
      "epoch": 0.9580006942034016,
      "grad_norm": 3.921875,
      "learning_rate": 0.0001041932796445432,
      "loss": 4.2238,
      "step": 3450
    },
    {
      "epoch": 0.9607775078097883,
      "grad_norm": 3.71875,
      "learning_rate": 0.00010391557900583171,
      "loss": 4.025,
      "step": 3460
    },
    {
      "epoch": 0.9635543214161749,
      "grad_norm": 8.375,
      "learning_rate": 0.00010363787836712025,
      "loss": 4.3391,
      "step": 3470
    },
    {
      "epoch": 0.9663311350225616,
      "grad_norm": 3.609375,
      "learning_rate": 0.00010336017772840879,
      "loss": 4.2848,
      "step": 3480
    },
    {
      "epoch": 0.9691079486289483,
      "grad_norm": 4.5625,
      "learning_rate": 0.00010308247708969731,
      "loss": 4.0656,
      "step": 3490
    },
    {
      "epoch": 0.9718847622353349,
      "grad_norm": 3.71875,
      "learning_rate": 0.00010280477645098585,
      "loss": 4.1727,
      "step": 3500
    },
    {
      "epoch": 0.9746615758417216,
      "grad_norm": 3.1875,
      "learning_rate": 0.00010252707581227436,
      "loss": 4.2266,
      "step": 3510
    },
    {
      "epoch": 0.9774383894481083,
      "grad_norm": 4.6875,
      "learning_rate": 0.0001022493751735629,
      "loss": 4.452,
      "step": 3520
    },
    {
      "epoch": 0.9802152030544949,
      "grad_norm": 4.09375,
      "learning_rate": 0.00010197167453485144,
      "loss": 4.2336,
      "step": 3530
    },
    {
      "epoch": 0.9829920166608817,
      "grad_norm": 4.65625,
      "learning_rate": 0.00010169397389613997,
      "loss": 4.2863,
      "step": 3540
    },
    {
      "epoch": 0.9857688302672684,
      "grad_norm": 5.53125,
      "learning_rate": 0.0001014162732574285,
      "loss": 4.2074,
      "step": 3550
    },
    {
      "epoch": 0.988545643873655,
      "grad_norm": 4.84375,
      "learning_rate": 0.00010113857261871702,
      "loss": 4.2609,
      "step": 3560
    },
    {
      "epoch": 0.9913224574800417,
      "grad_norm": 4.75,
      "learning_rate": 0.00010086087198000556,
      "loss": 4.232,
      "step": 3570
    },
    {
      "epoch": 0.9940992710864284,
      "grad_norm": 3.90625,
      "learning_rate": 0.0001005831713412941,
      "loss": 4.0773,
      "step": 3580
    },
    {
      "epoch": 0.996876084692815,
      "grad_norm": 4.5625,
      "learning_rate": 0.00010030547070258263,
      "loss": 4.1648,
      "step": 3590
    },
    {
      "epoch": 0.9996528982992017,
      "grad_norm": 3.984375,
      "learning_rate": 0.00010002777006387116,
      "loss": 4.1918,
      "step": 3600
    },
    {
      "epoch": 1.0022214508851093,
      "grad_norm": 3.8125,
      "learning_rate": 9.975006942515968e-05,
      "loss": 3.7516,
      "step": 3610
    },
    {
      "epoch": 1.004998264491496,
      "grad_norm": 4.84375,
      "learning_rate": 9.947236878644821e-05,
      "loss": 4.0508,
      "step": 3620
    },
    {
      "epoch": 1.0077750780978827,
      "grad_norm": 4.875,
      "learning_rate": 9.919466814773675e-05,
      "loss": 3.7129,
      "step": 3630
    },
    {
      "epoch": 1.0105518917042693,
      "grad_norm": 5.4375,
      "learning_rate": 9.891696750902527e-05,
      "loss": 3.8746,
      "step": 3640
    },
    {
      "epoch": 1.013328705310656,
      "grad_norm": 4.28125,
      "learning_rate": 9.863926687031381e-05,
      "loss": 4.1512,
      "step": 3650
    },
    {
      "epoch": 1.0161055189170427,
      "grad_norm": 3.6875,
      "learning_rate": 9.836156623160234e-05,
      "loss": 3.793,
      "step": 3660
    },
    {
      "epoch": 1.0188823325234293,
      "grad_norm": 4.40625,
      "learning_rate": 9.808386559289086e-05,
      "loss": 4.0793,
      "step": 3670
    },
    {
      "epoch": 1.0216591461298161,
      "grad_norm": 4.1875,
      "learning_rate": 9.78061649541794e-05,
      "loss": 3.9824,
      "step": 3680
    },
    {
      "epoch": 1.0244359597362027,
      "grad_norm": 4.21875,
      "learning_rate": 9.752846431546793e-05,
      "loss": 4.0324,
      "step": 3690
    },
    {
      "epoch": 1.0272127733425893,
      "grad_norm": 4.4375,
      "learning_rate": 9.725076367675647e-05,
      "loss": 4.0652,
      "step": 3700
    },
    {
      "epoch": 1.0299895869489761,
      "grad_norm": 3.765625,
      "learning_rate": 9.6973063038045e-05,
      "loss": 3.9711,
      "step": 3710
    },
    {
      "epoch": 1.0327664005553627,
      "grad_norm": 4.3125,
      "learning_rate": 9.669536239933352e-05,
      "loss": 4.0145,
      "step": 3720
    },
    {
      "epoch": 1.0355432141617493,
      "grad_norm": 7.25,
      "learning_rate": 9.641766176062206e-05,
      "loss": 4.0676,
      "step": 3730
    },
    {
      "epoch": 1.0383200277681361,
      "grad_norm": 5.125,
      "learning_rate": 9.613996112191058e-05,
      "loss": 4.0332,
      "step": 3740
    },
    {
      "epoch": 1.0410968413745227,
      "grad_norm": 4.6875,
      "learning_rate": 9.586226048319912e-05,
      "loss": 4.1996,
      "step": 3750
    },
    {
      "epoch": 1.0438736549809093,
      "grad_norm": 4.3125,
      "learning_rate": 9.558455984448765e-05,
      "loss": 3.8832,
      "step": 3760
    },
    {
      "epoch": 1.0466504685872962,
      "grad_norm": 4.03125,
      "learning_rate": 9.530685920577617e-05,
      "loss": 4.1195,
      "step": 3770
    },
    {
      "epoch": 1.0494272821936828,
      "grad_norm": 4.4375,
      "learning_rate": 9.502915856706471e-05,
      "loss": 4.1273,
      "step": 3780
    },
    {
      "epoch": 1.0522040958000694,
      "grad_norm": 4.5,
      "learning_rate": 9.475145792835324e-05,
      "loss": 3.9918,
      "step": 3790
    },
    {
      "epoch": 1.0549809094064562,
      "grad_norm": 4.28125,
      "learning_rate": 9.447375728964178e-05,
      "loss": 3.7836,
      "step": 3800
    },
    {
      "epoch": 1.0577577230128428,
      "grad_norm": 4.6875,
      "learning_rate": 9.41960566509303e-05,
      "loss": 3.8883,
      "step": 3810
    },
    {
      "epoch": 1.0605345366192294,
      "grad_norm": 3.421875,
      "learning_rate": 9.391835601221883e-05,
      "loss": 3.8371,
      "step": 3820
    },
    {
      "epoch": 1.0633113502256162,
      "grad_norm": 5.0,
      "learning_rate": 9.364065537350737e-05,
      "loss": 4.0719,
      "step": 3830
    },
    {
      "epoch": 1.0660881638320028,
      "grad_norm": 5.0,
      "learning_rate": 9.336295473479589e-05,
      "loss": 4.0328,
      "step": 3840
    },
    {
      "epoch": 1.0688649774383894,
      "grad_norm": 6.1875,
      "learning_rate": 9.308525409608443e-05,
      "loss": 3.9379,
      "step": 3850
    },
    {
      "epoch": 1.0716417910447762,
      "grad_norm": 3.703125,
      "learning_rate": 9.280755345737296e-05,
      "loss": 4.1672,
      "step": 3860
    },
    {
      "epoch": 1.0744186046511628,
      "grad_norm": 4.28125,
      "learning_rate": 9.252985281866148e-05,
      "loss": 4.032,
      "step": 3870
    },
    {
      "epoch": 1.0771954182575494,
      "grad_norm": 4.03125,
      "learning_rate": 9.225215217995001e-05,
      "loss": 3.9898,
      "step": 3880
    },
    {
      "epoch": 1.0799722318639362,
      "grad_norm": 3.484375,
      "learning_rate": 9.197445154123855e-05,
      "loss": 3.9477,
      "step": 3890
    },
    {
      "epoch": 1.0827490454703228,
      "grad_norm": 5.15625,
      "learning_rate": 9.169675090252709e-05,
      "loss": 4.148,
      "step": 3900
    },
    {
      "epoch": 1.0855258590767094,
      "grad_norm": 3.75,
      "learning_rate": 9.141905026381561e-05,
      "loss": 4.0406,
      "step": 3910
    },
    {
      "epoch": 1.0883026726830962,
      "grad_norm": 3.640625,
      "learning_rate": 9.114134962510415e-05,
      "loss": 3.991,
      "step": 3920
    },
    {
      "epoch": 1.0910794862894828,
      "grad_norm": 3.3125,
      "learning_rate": 9.086364898639266e-05,
      "loss": 4.0094,
      "step": 3930
    },
    {
      "epoch": 1.0938562998958694,
      "grad_norm": 4.28125,
      "learning_rate": 9.05859483476812e-05,
      "loss": 3.9988,
      "step": 3940
    },
    {
      "epoch": 1.0966331135022562,
      "grad_norm": 4.03125,
      "learning_rate": 9.030824770896974e-05,
      "loss": 4.0598,
      "step": 3950
    },
    {
      "epoch": 1.0994099271086428,
      "grad_norm": 3.90625,
      "learning_rate": 9.003054707025827e-05,
      "loss": 3.9414,
      "step": 3960
    },
    {
      "epoch": 1.1021867407150294,
      "grad_norm": 4.4375,
      "learning_rate": 8.97528464315468e-05,
      "loss": 4.0855,
      "step": 3970
    },
    {
      "epoch": 1.1049635543214162,
      "grad_norm": 3.765625,
      "learning_rate": 8.947514579283533e-05,
      "loss": 3.8734,
      "step": 3980
    },
    {
      "epoch": 1.1077403679278028,
      "grad_norm": 5.09375,
      "learning_rate": 8.919744515412386e-05,
      "loss": 4.1789,
      "step": 3990
    },
    {
      "epoch": 1.1105171815341894,
      "grad_norm": 3.796875,
      "learning_rate": 8.891974451541238e-05,
      "loss": 4.0191,
      "step": 4000
    },
    {
      "epoch": 1.1132939951405763,
      "grad_norm": 4.40625,
      "learning_rate": 8.864204387670092e-05,
      "loss": 4.0492,
      "step": 4010
    },
    {
      "epoch": 1.1160708087469629,
      "grad_norm": 5.3125,
      "learning_rate": 8.836434323798946e-05,
      "loss": 4.1531,
      "step": 4020
    },
    {
      "epoch": 1.1188476223533494,
      "grad_norm": 4.375,
      "learning_rate": 8.808664259927798e-05,
      "loss": 4.1234,
      "step": 4030
    },
    {
      "epoch": 1.1216244359597363,
      "grad_norm": 4.34375,
      "learning_rate": 8.780894196056651e-05,
      "loss": 4.1855,
      "step": 4040
    },
    {
      "epoch": 1.1244012495661229,
      "grad_norm": 3.875,
      "learning_rate": 8.753124132185504e-05,
      "loss": 4.0203,
      "step": 4050
    },
    {
      "epoch": 1.1271780631725095,
      "grad_norm": 4.375,
      "learning_rate": 8.725354068314357e-05,
      "loss": 3.9609,
      "step": 4060
    },
    {
      "epoch": 1.1299548767788963,
      "grad_norm": 3.953125,
      "learning_rate": 8.697584004443211e-05,
      "loss": 4.0023,
      "step": 4070
    },
    {
      "epoch": 1.1327316903852829,
      "grad_norm": 4.5,
      "learning_rate": 8.669813940572064e-05,
      "loss": 3.777,
      "step": 4080
    },
    {
      "epoch": 1.1355085039916695,
      "grad_norm": 4.15625,
      "learning_rate": 8.642043876700916e-05,
      "loss": 4.0875,
      "step": 4090
    },
    {
      "epoch": 1.1382853175980563,
      "grad_norm": 4.4375,
      "learning_rate": 8.614273812829769e-05,
      "loss": 3.95,
      "step": 4100
    },
    {
      "epoch": 1.1410621312044429,
      "grad_norm": 3.921875,
      "learning_rate": 8.586503748958623e-05,
      "loss": 4.2004,
      "step": 4110
    },
    {
      "epoch": 1.1438389448108295,
      "grad_norm": 3.75,
      "learning_rate": 8.558733685087477e-05,
      "loss": 4.098,
      "step": 4120
    },
    {
      "epoch": 1.1466157584172163,
      "grad_norm": 4.59375,
      "learning_rate": 8.53096362121633e-05,
      "loss": 3.9379,
      "step": 4130
    },
    {
      "epoch": 1.149392572023603,
      "grad_norm": 4.375,
      "learning_rate": 8.503193557345183e-05,
      "loss": 4.0387,
      "step": 4140
    },
    {
      "epoch": 1.1521693856299895,
      "grad_norm": 4.21875,
      "learning_rate": 8.475423493474034e-05,
      "loss": 3.9648,
      "step": 4150
    },
    {
      "epoch": 1.1549461992363763,
      "grad_norm": 6.0,
      "learning_rate": 8.447653429602888e-05,
      "loss": 3.9684,
      "step": 4160
    },
    {
      "epoch": 1.157723012842763,
      "grad_norm": 4.96875,
      "learning_rate": 8.419883365731741e-05,
      "loss": 4.0953,
      "step": 4170
    },
    {
      "epoch": 1.1604998264491495,
      "grad_norm": 4.15625,
      "learning_rate": 8.392113301860595e-05,
      "loss": 4.0945,
      "step": 4180
    },
    {
      "epoch": 1.1632766400555363,
      "grad_norm": 5.0625,
      "learning_rate": 8.364343237989449e-05,
      "loss": 3.802,
      "step": 4190
    },
    {
      "epoch": 1.166053453661923,
      "grad_norm": 5.375,
      "learning_rate": 8.336573174118301e-05,
      "loss": 3.9859,
      "step": 4200
    },
    {
      "epoch": 1.1688302672683095,
      "grad_norm": 3.625,
      "learning_rate": 8.308803110247154e-05,
      "loss": 3.9387,
      "step": 4210
    },
    {
      "epoch": 1.1716070808746963,
      "grad_norm": 3.953125,
      "learning_rate": 8.281033046376006e-05,
      "loss": 4.0602,
      "step": 4220
    },
    {
      "epoch": 1.174383894481083,
      "grad_norm": 3.953125,
      "learning_rate": 8.25326298250486e-05,
      "loss": 4.1035,
      "step": 4230
    },
    {
      "epoch": 1.1771607080874695,
      "grad_norm": 4.53125,
      "learning_rate": 8.225492918633714e-05,
      "loss": 4.2125,
      "step": 4240
    },
    {
      "epoch": 1.1799375216938564,
      "grad_norm": 3.78125,
      "learning_rate": 8.197722854762567e-05,
      "loss": 3.9719,
      "step": 4250
    },
    {
      "epoch": 1.182714335300243,
      "grad_norm": 4.03125,
      "learning_rate": 8.169952790891419e-05,
      "loss": 3.9602,
      "step": 4260
    },
    {
      "epoch": 1.1854911489066295,
      "grad_norm": 3.96875,
      "learning_rate": 8.142182727020272e-05,
      "loss": 3.9559,
      "step": 4270
    },
    {
      "epoch": 1.1882679625130164,
      "grad_norm": 4.625,
      "learning_rate": 8.114412663149126e-05,
      "loss": 4.0137,
      "step": 4280
    },
    {
      "epoch": 1.191044776119403,
      "grad_norm": 3.859375,
      "learning_rate": 8.086642599277978e-05,
      "loss": 3.8098,
      "step": 4290
    },
    {
      "epoch": 1.1938215897257898,
      "grad_norm": 4.5625,
      "learning_rate": 8.058872535406832e-05,
      "loss": 4.0078,
      "step": 4300
    },
    {
      "epoch": 1.1965984033321764,
      "grad_norm": 3.765625,
      "learning_rate": 8.031102471535685e-05,
      "loss": 3.8676,
      "step": 4310
    },
    {
      "epoch": 1.199375216938563,
      "grad_norm": 4.71875,
      "learning_rate": 8.003332407664537e-05,
      "loss": 3.8438,
      "step": 4320
    },
    {
      "epoch": 1.2021520305449496,
      "grad_norm": 4.53125,
      "learning_rate": 7.975562343793391e-05,
      "loss": 3.9434,
      "step": 4330
    },
    {
      "epoch": 1.2049288441513364,
      "grad_norm": 4.34375,
      "learning_rate": 7.947792279922244e-05,
      "loss": 4.0941,
      "step": 4340
    },
    {
      "epoch": 1.207705657757723,
      "grad_norm": 4.4375,
      "learning_rate": 7.920022216051098e-05,
      "loss": 3.9242,
      "step": 4350
    },
    {
      "epoch": 1.2104824713641098,
      "grad_norm": 7.65625,
      "learning_rate": 7.89225215217995e-05,
      "loss": 4.118,
      "step": 4360
    },
    {
      "epoch": 1.2132592849704964,
      "grad_norm": 4.625,
      "learning_rate": 7.864482088308803e-05,
      "loss": 3.9027,
      "step": 4370
    },
    {
      "epoch": 1.216036098576883,
      "grad_norm": 5.15625,
      "learning_rate": 7.836712024437657e-05,
      "loss": 3.968,
      "step": 4380
    },
    {
      "epoch": 1.2188129121832696,
      "grad_norm": 4.65625,
      "learning_rate": 7.808941960566509e-05,
      "loss": 4.052,
      "step": 4390
    },
    {
      "epoch": 1.2215897257896564,
      "grad_norm": 6.40625,
      "learning_rate": 7.781171896695363e-05,
      "loss": 4.1219,
      "step": 4400
    },
    {
      "epoch": 1.224366539396043,
      "grad_norm": 5.8125,
      "learning_rate": 7.753401832824217e-05,
      "loss": 4.0496,
      "step": 4410
    },
    {
      "epoch": 1.2271433530024298,
      "grad_norm": 4.5,
      "learning_rate": 7.72563176895307e-05,
      "loss": 3.9141,
      "step": 4420
    },
    {
      "epoch": 1.2299201666088164,
      "grad_norm": 4.5,
      "learning_rate": 7.697861705081922e-05,
      "loss": 3.9867,
      "step": 4430
    },
    {
      "epoch": 1.232696980215203,
      "grad_norm": 4.3125,
      "learning_rate": 7.670091641210775e-05,
      "loss": 4.0402,
      "step": 4440
    },
    {
      "epoch": 1.2354737938215896,
      "grad_norm": 4.34375,
      "learning_rate": 7.642321577339629e-05,
      "loss": 3.8941,
      "step": 4450
    },
    {
      "epoch": 1.2382506074279764,
      "grad_norm": 5.46875,
      "learning_rate": 7.614551513468481e-05,
      "loss": 4.0195,
      "step": 4460
    },
    {
      "epoch": 1.241027421034363,
      "grad_norm": 4.3125,
      "learning_rate": 7.586781449597335e-05,
      "loss": 4.0668,
      "step": 4470
    },
    {
      "epoch": 1.2438042346407499,
      "grad_norm": 3.90625,
      "learning_rate": 7.559011385726188e-05,
      "loss": 4.2402,
      "step": 4480
    },
    {
      "epoch": 1.2465810482471364,
      "grad_norm": 6.625,
      "learning_rate": 7.53124132185504e-05,
      "loss": 3.9855,
      "step": 4490
    },
    {
      "epoch": 1.249357861853523,
      "grad_norm": 3.890625,
      "learning_rate": 7.503471257983894e-05,
      "loss": 3.9723,
      "step": 4500
    },
    {
      "epoch": 1.2521346754599096,
      "grad_norm": 3.921875,
      "learning_rate": 7.475701194112747e-05,
      "loss": 4.0426,
      "step": 4510
    },
    {
      "epoch": 1.2549114890662965,
      "grad_norm": 4.625,
      "learning_rate": 7.4479311302416e-05,
      "loss": 3.9453,
      "step": 4520
    },
    {
      "epoch": 1.257688302672683,
      "grad_norm": 4.25,
      "learning_rate": 7.420161066370453e-05,
      "loss": 4.0547,
      "step": 4530
    },
    {
      "epoch": 1.2604651162790699,
      "grad_norm": 4.03125,
      "learning_rate": 7.392391002499306e-05,
      "loss": 3.9496,
      "step": 4540
    },
    {
      "epoch": 1.2632419298854565,
      "grad_norm": 4.40625,
      "learning_rate": 7.36462093862816e-05,
      "loss": 4.177,
      "step": 4550
    },
    {
      "epoch": 1.266018743491843,
      "grad_norm": 5.3125,
      "learning_rate": 7.336850874757012e-05,
      "loss": 4.1191,
      "step": 4560
    },
    {
      "epoch": 1.2687955570982297,
      "grad_norm": 4.6875,
      "learning_rate": 7.309080810885866e-05,
      "loss": 3.9949,
      "step": 4570
    },
    {
      "epoch": 1.2715723707046165,
      "grad_norm": 4.5,
      "learning_rate": 7.281310747014718e-05,
      "loss": 4.0422,
      "step": 4580
    },
    {
      "epoch": 1.274349184311003,
      "grad_norm": 8.125,
      "learning_rate": 7.253540683143571e-05,
      "loss": 4.1129,
      "step": 4590
    },
    {
      "epoch": 1.27712599791739,
      "grad_norm": 4.40625,
      "learning_rate": 7.225770619272425e-05,
      "loss": 3.8582,
      "step": 4600
    },
    {
      "epoch": 1.2799028115237765,
      "grad_norm": 3.875,
      "learning_rate": 7.198000555401277e-05,
      "loss": 3.943,
      "step": 4610
    },
    {
      "epoch": 1.282679625130163,
      "grad_norm": 4.25,
      "learning_rate": 7.170230491530131e-05,
      "loss": 4.1688,
      "step": 4620
    },
    {
      "epoch": 1.2854564387365497,
      "grad_norm": 4.3125,
      "learning_rate": 7.142460427658984e-05,
      "loss": 4.0203,
      "step": 4630
    },
    {
      "epoch": 1.2882332523429365,
      "grad_norm": 5.09375,
      "learning_rate": 7.114690363787836e-05,
      "loss": 4.0582,
      "step": 4640
    },
    {
      "epoch": 1.291010065949323,
      "grad_norm": 4.5625,
      "learning_rate": 7.08692029991669e-05,
      "loss": 4.0988,
      "step": 4650
    },
    {
      "epoch": 1.29378687955571,
      "grad_norm": 3.984375,
      "learning_rate": 7.059150236045543e-05,
      "loss": 4.0852,
      "step": 4660
    },
    {
      "epoch": 1.2965636931620965,
      "grad_norm": 3.359375,
      "learning_rate": 7.031380172174397e-05,
      "loss": 3.9723,
      "step": 4670
    },
    {
      "epoch": 1.2993405067684831,
      "grad_norm": 4.71875,
      "learning_rate": 7.003610108303249e-05,
      "loss": 3.8207,
      "step": 4680
    },
    {
      "epoch": 1.3021173203748697,
      "grad_norm": 4.5625,
      "learning_rate": 6.975840044432103e-05,
      "loss": 4.132,
      "step": 4690
    },
    {
      "epoch": 1.3048941339812565,
      "grad_norm": 4.125,
      "learning_rate": 6.948069980560956e-05,
      "loss": 3.8164,
      "step": 4700
    },
    {
      "epoch": 1.3076709475876431,
      "grad_norm": 5.78125,
      "learning_rate": 6.920299916689808e-05,
      "loss": 3.9977,
      "step": 4710
    },
    {
      "epoch": 1.31044776119403,
      "grad_norm": 5.65625,
      "learning_rate": 6.892529852818662e-05,
      "loss": 3.9207,
      "step": 4720
    },
    {
      "epoch": 1.3132245748004165,
      "grad_norm": 4.5,
      "learning_rate": 6.864759788947515e-05,
      "loss": 4.0723,
      "step": 4730
    },
    {
      "epoch": 1.3160013884068031,
      "grad_norm": 8.375,
      "learning_rate": 6.836989725076369e-05,
      "loss": 3.8898,
      "step": 4740
    },
    {
      "epoch": 1.3187782020131897,
      "grad_norm": 3.515625,
      "learning_rate": 6.809219661205221e-05,
      "loss": 3.8328,
      "step": 4750
    },
    {
      "epoch": 1.3215550156195766,
      "grad_norm": 7.53125,
      "learning_rate": 6.781449597334074e-05,
      "loss": 3.9207,
      "step": 4760
    },
    {
      "epoch": 1.3243318292259632,
      "grad_norm": 5.34375,
      "learning_rate": 6.753679533462928e-05,
      "loss": 4.0145,
      "step": 4770
    },
    {
      "epoch": 1.32710864283235,
      "grad_norm": 5.53125,
      "learning_rate": 6.72590946959178e-05,
      "loss": 3.9805,
      "step": 4780
    },
    {
      "epoch": 1.3298854564387366,
      "grad_norm": 5.75,
      "learning_rate": 6.698139405720634e-05,
      "loss": 4.0422,
      "step": 4790
    },
    {
      "epoch": 1.3326622700451232,
      "grad_norm": 4.59375,
      "learning_rate": 6.670369341849487e-05,
      "loss": 3.957,
      "step": 4800
    },
    {
      "epoch": 1.3354390836515098,
      "grad_norm": 3.734375,
      "learning_rate": 6.642599277978339e-05,
      "loss": 3.9699,
      "step": 4810
    },
    {
      "epoch": 1.3382158972578966,
      "grad_norm": 4.6875,
      "learning_rate": 6.614829214107193e-05,
      "loss": 4.1184,
      "step": 4820
    },
    {
      "epoch": 1.3409927108642832,
      "grad_norm": 3.875,
      "learning_rate": 6.587059150236046e-05,
      "loss": 4.0359,
      "step": 4830
    },
    {
      "epoch": 1.34376952447067,
      "grad_norm": 5.0,
      "learning_rate": 6.5592890863649e-05,
      "loss": 3.8727,
      "step": 4840
    },
    {
      "epoch": 1.3465463380770566,
      "grad_norm": 5.59375,
      "learning_rate": 6.531519022493752e-05,
      "loss": 3.9969,
      "step": 4850
    },
    {
      "epoch": 1.3493231516834432,
      "grad_norm": 4.34375,
      "learning_rate": 6.503748958622605e-05,
      "loss": 4.3609,
      "step": 4860
    },
    {
      "epoch": 1.3520999652898298,
      "grad_norm": 4.15625,
      "learning_rate": 6.475978894751457e-05,
      "loss": 3.9723,
      "step": 4870
    },
    {
      "epoch": 1.3548767788962166,
      "grad_norm": 5.34375,
      "learning_rate": 6.448208830880311e-05,
      "loss": 4.1641,
      "step": 4880
    },
    {
      "epoch": 1.3576535925026032,
      "grad_norm": 5.59375,
      "learning_rate": 6.420438767009165e-05,
      "loss": 3.8207,
      "step": 4890
    },
    {
      "epoch": 1.36043040610899,
      "grad_norm": 4.40625,
      "learning_rate": 6.392668703138018e-05,
      "loss": 3.9371,
      "step": 4900
    },
    {
      "epoch": 1.3632072197153766,
      "grad_norm": 4.75,
      "learning_rate": 6.364898639266871e-05,
      "loss": 4.0465,
      "step": 4910
    },
    {
      "epoch": 1.3659840333217632,
      "grad_norm": 4.75,
      "learning_rate": 6.337128575395723e-05,
      "loss": 4.0992,
      "step": 4920
    },
    {
      "epoch": 1.3687608469281498,
      "grad_norm": 3.859375,
      "learning_rate": 6.309358511524577e-05,
      "loss": 3.898,
      "step": 4930
    },
    {
      "epoch": 1.3715376605345366,
      "grad_norm": 4.09375,
      "learning_rate": 6.28158844765343e-05,
      "loss": 4.0547,
      "step": 4940
    },
    {
      "epoch": 1.3743144741409232,
      "grad_norm": 5.0625,
      "learning_rate": 6.253818383782283e-05,
      "loss": 3.9855,
      "step": 4950
    },
    {
      "epoch": 1.37709128774731,
      "grad_norm": 4.8125,
      "learning_rate": 6.226048319911137e-05,
      "loss": 3.8664,
      "step": 4960
    },
    {
      "epoch": 1.3798681013536966,
      "grad_norm": 4.75,
      "learning_rate": 6.19827825603999e-05,
      "loss": 4.1773,
      "step": 4970
    },
    {
      "epoch": 1.3826449149600832,
      "grad_norm": 4.21875,
      "learning_rate": 6.170508192168842e-05,
      "loss": 4.0082,
      "step": 4980
    },
    {
      "epoch": 1.38542172856647,
      "grad_norm": 3.5,
      "learning_rate": 6.142738128297696e-05,
      "loss": 3.9441,
      "step": 4990
    },
    {
      "epoch": 1.3881985421728567,
      "grad_norm": 3.59375,
      "learning_rate": 6.114968064426548e-05,
      "loss": 3.8566,
      "step": 5000
    },
    {
      "epoch": 1.3909753557792432,
      "grad_norm": 4.625,
      "learning_rate": 6.087198000555402e-05,
      "loss": 3.9937,
      "step": 5010
    },
    {
      "epoch": 1.39375216938563,
      "grad_norm": 3.796875,
      "learning_rate": 6.059427936684254e-05,
      "loss": 4.0344,
      "step": 5020
    },
    {
      "epoch": 1.3965289829920167,
      "grad_norm": 5.09375,
      "learning_rate": 6.031657872813108e-05,
      "loss": 3.9359,
      "step": 5030
    },
    {
      "epoch": 1.3993057965984033,
      "grad_norm": 5.21875,
      "learning_rate": 6.0038878089419607e-05,
      "loss": 4.0551,
      "step": 5040
    },
    {
      "epoch": 1.40208261020479,
      "grad_norm": 3.59375,
      "learning_rate": 5.976117745070814e-05,
      "loss": 3.8203,
      "step": 5050
    },
    {
      "epoch": 1.4048594238111767,
      "grad_norm": 4.5,
      "learning_rate": 5.948347681199668e-05,
      "loss": 3.9918,
      "step": 5060
    },
    {
      "epoch": 1.4076362374175633,
      "grad_norm": 4.1875,
      "learning_rate": 5.9205776173285197e-05,
      "loss": 4.1148,
      "step": 5070
    },
    {
      "epoch": 1.41041305102395,
      "grad_norm": 4.46875,
      "learning_rate": 5.8928075534573736e-05,
      "loss": 4.1797,
      "step": 5080
    },
    {
      "epoch": 1.4131898646303367,
      "grad_norm": 3.875,
      "learning_rate": 5.865037489586226e-05,
      "loss": 3.8527,
      "step": 5090
    },
    {
      "epoch": 1.4159666782367233,
      "grad_norm": 4.875,
      "learning_rate": 5.837267425715079e-05,
      "loss": 3.9684,
      "step": 5100
    },
    {
      "epoch": 1.41874349184311,
      "grad_norm": 4.09375,
      "learning_rate": 5.809497361843933e-05,
      "loss": 3.8215,
      "step": 5110
    },
    {
      "epoch": 1.4215203054494967,
      "grad_norm": 4.8125,
      "learning_rate": 5.781727297972786e-05,
      "loss": 3.8754,
      "step": 5120
    },
    {
      "epoch": 1.4242971190558833,
      "grad_norm": 4.6875,
      "learning_rate": 5.753957234101639e-05,
      "loss": 4.0695,
      "step": 5130
    },
    {
      "epoch": 1.4270739326622701,
      "grad_norm": 3.6875,
      "learning_rate": 5.7261871702304916e-05,
      "loss": 3.9438,
      "step": 5140
    },
    {
      "epoch": 1.4298507462686567,
      "grad_norm": 4.875,
      "learning_rate": 5.698417106359345e-05,
      "loss": 3.9102,
      "step": 5150
    },
    {
      "epoch": 1.4326275598750433,
      "grad_norm": 5.84375,
      "learning_rate": 5.670647042488197e-05,
      "loss": 4.1352,
      "step": 5160
    },
    {
      "epoch": 1.4354043734814301,
      "grad_norm": 4.09375,
      "learning_rate": 5.642876978617051e-05,
      "loss": 3.941,
      "step": 5170
    },
    {
      "epoch": 1.4381811870878167,
      "grad_norm": 4.40625,
      "learning_rate": 5.6151069147459045e-05,
      "loss": 3.893,
      "step": 5180
    },
    {
      "epoch": 1.4409580006942033,
      "grad_norm": 5.09375,
      "learning_rate": 5.587336850874757e-05,
      "loss": 4.1145,
      "step": 5190
    },
    {
      "epoch": 1.4437348143005901,
      "grad_norm": 5.03125,
      "learning_rate": 5.55956678700361e-05,
      "loss": 3.9898,
      "step": 5200
    },
    {
      "epoch": 1.4465116279069767,
      "grad_norm": 4.59375,
      "learning_rate": 5.531796723132463e-05,
      "loss": 3.9555,
      "step": 5210
    },
    {
      "epoch": 1.4492884415133633,
      "grad_norm": 3.921875,
      "learning_rate": 5.504026659261317e-05,
      "loss": 4.2996,
      "step": 5220
    },
    {
      "epoch": 1.4520652551197502,
      "grad_norm": 4.84375,
      "learning_rate": 5.47625659539017e-05,
      "loss": 4.1117,
      "step": 5230
    },
    {
      "epoch": 1.4548420687261368,
      "grad_norm": 4.65625,
      "learning_rate": 5.4484865315190225e-05,
      "loss": 4.0844,
      "step": 5240
    },
    {
      "epoch": 1.4576188823325233,
      "grad_norm": 7.9375,
      "learning_rate": 5.4207164676478764e-05,
      "loss": 3.9262,
      "step": 5250
    },
    {
      "epoch": 1.4603956959389102,
      "grad_norm": 4.90625,
      "learning_rate": 5.392946403776728e-05,
      "loss": 3.8992,
      "step": 5260
    },
    {
      "epoch": 1.4631725095452968,
      "grad_norm": 4.6875,
      "learning_rate": 5.365176339905582e-05,
      "loss": 3.9895,
      "step": 5270
    },
    {
      "epoch": 1.4659493231516834,
      "grad_norm": 4.21875,
      "learning_rate": 5.3374062760344354e-05,
      "loss": 3.9691,
      "step": 5280
    },
    {
      "epoch": 1.4687261367580702,
      "grad_norm": 5.34375,
      "learning_rate": 5.309636212163288e-05,
      "loss": 4.048,
      "step": 5290
    },
    {
      "epoch": 1.4715029503644568,
      "grad_norm": 4.4375,
      "learning_rate": 5.281866148292142e-05,
      "loss": 4.0211,
      "step": 5300
    },
    {
      "epoch": 1.4742797639708436,
      "grad_norm": 4.25,
      "learning_rate": 5.2540960844209943e-05,
      "loss": 3.8039,
      "step": 5310
    },
    {
      "epoch": 1.4770565775772302,
      "grad_norm": 5.65625,
      "learning_rate": 5.2263260205498476e-05,
      "loss": 3.9051,
      "step": 5320
    },
    {
      "epoch": 1.4798333911836168,
      "grad_norm": 5.375,
      "learning_rate": 5.1985559566787e-05,
      "loss": 3.9965,
      "step": 5330
    },
    {
      "epoch": 1.4826102047900034,
      "grad_norm": 5.96875,
      "learning_rate": 5.1707858928075533e-05,
      "loss": 3.9328,
      "step": 5340
    },
    {
      "epoch": 1.4853870183963902,
      "grad_norm": 11.0625,
      "learning_rate": 5.143015828936407e-05,
      "loss": 4.1578,
      "step": 5350
    },
    {
      "epoch": 1.4881638320027768,
      "grad_norm": 6.34375,
      "learning_rate": 5.11524576506526e-05,
      "loss": 4.0457,
      "step": 5360
    },
    {
      "epoch": 1.4909406456091636,
      "grad_norm": 4.8125,
      "learning_rate": 5.087475701194113e-05,
      "loss": 3.8828,
      "step": 5370
    },
    {
      "epoch": 1.4937174592155502,
      "grad_norm": 4.5,
      "learning_rate": 5.0597056373229656e-05,
      "loss": 3.9477,
      "step": 5380
    },
    {
      "epoch": 1.4964942728219368,
      "grad_norm": 4.25,
      "learning_rate": 5.0319355734518195e-05,
      "loss": 4.1113,
      "step": 5390
    },
    {
      "epoch": 1.4992710864283234,
      "grad_norm": 4.0625,
      "learning_rate": 5.004165509580673e-05,
      "loss": 4.0148,
      "step": 5400
    },
    {
      "epoch": 1.50204790003471,
      "grad_norm": 4.53125,
      "learning_rate": 4.976395445709525e-05,
      "loss": 3.8957,
      "step": 5410
    },
    {
      "epoch": 1.5048247136410968,
      "grad_norm": 4.875,
      "learning_rate": 4.9486253818383785e-05,
      "loss": 3.8902,
      "step": 5420
    },
    {
      "epoch": 1.5076015272474836,
      "grad_norm": 5.625,
      "learning_rate": 4.920855317967232e-05,
      "loss": 3.884,
      "step": 5430
    },
    {
      "epoch": 1.5103783408538702,
      "grad_norm": 4.78125,
      "learning_rate": 4.893085254096085e-05,
      "loss": 3.7961,
      "step": 5440
    },
    {
      "epoch": 1.5131551544602568,
      "grad_norm": 4.75,
      "learning_rate": 4.8653151902249375e-05,
      "loss": 3.8762,
      "step": 5450
    },
    {
      "epoch": 1.5159319680666434,
      "grad_norm": 4.3125,
      "learning_rate": 4.837545126353791e-05,
      "loss": 3.9773,
      "step": 5460
    },
    {
      "epoch": 1.5187087816730303,
      "grad_norm": 4.21875,
      "learning_rate": 4.809775062482644e-05,
      "loss": 3.975,
      "step": 5470
    },
    {
      "epoch": 1.5214855952794168,
      "grad_norm": 3.765625,
      "learning_rate": 4.782004998611497e-05,
      "loss": 4.0402,
      "step": 5480
    },
    {
      "epoch": 1.5242624088858037,
      "grad_norm": 4.4375,
      "learning_rate": 4.7542349347403504e-05,
      "loss": 4.0457,
      "step": 5490
    },
    {
      "epoch": 1.5270392224921903,
      "grad_norm": 5.96875,
      "learning_rate": 4.7264648708692036e-05,
      "loss": 3.9363,
      "step": 5500
    },
    {
      "epoch": 1.5298160360985769,
      "grad_norm": 4.21875,
      "learning_rate": 4.698694806998056e-05,
      "loss": 4.0195,
      "step": 5510
    },
    {
      "epoch": 1.5325928497049635,
      "grad_norm": 5.0625,
      "learning_rate": 4.6709247431269094e-05,
      "loss": 3.9543,
      "step": 5520
    },
    {
      "epoch": 1.5353696633113503,
      "grad_norm": 5.375,
      "learning_rate": 4.6431546792557626e-05,
      "loss": 3.9277,
      "step": 5530
    },
    {
      "epoch": 1.5381464769177369,
      "grad_norm": 4.25,
      "learning_rate": 4.615384615384616e-05,
      "loss": 3.9359,
      "step": 5540
    },
    {
      "epoch": 1.5409232905241237,
      "grad_norm": 4.3125,
      "learning_rate": 4.587614551513469e-05,
      "loss": 4.0187,
      "step": 5550
    },
    {
      "epoch": 1.5437001041305103,
      "grad_norm": 4.34375,
      "learning_rate": 4.5598444876423216e-05,
      "loss": 3.8766,
      "step": 5560
    },
    {
      "epoch": 1.5464769177368969,
      "grad_norm": 4.375,
      "learning_rate": 4.532074423771175e-05,
      "loss": 4.0355,
      "step": 5570
    },
    {
      "epoch": 1.5492537313432835,
      "grad_norm": 5.71875,
      "learning_rate": 4.504304359900028e-05,
      "loss": 3.8039,
      "step": 5580
    },
    {
      "epoch": 1.5520305449496703,
      "grad_norm": 4.21875,
      "learning_rate": 4.4765342960288806e-05,
      "loss": 3.8941,
      "step": 5590
    },
    {
      "epoch": 1.554807358556057,
      "grad_norm": 4.25,
      "learning_rate": 4.4487642321577345e-05,
      "loss": 3.9438,
      "step": 5600
    },
    {
      "epoch": 1.5575841721624437,
      "grad_norm": 5.0,
      "learning_rate": 4.420994168286588e-05,
      "loss": 3.8789,
      "step": 5610
    },
    {
      "epoch": 1.5603609857688303,
      "grad_norm": 4.59375,
      "learning_rate": 4.39322410441544e-05,
      "loss": 3.9781,
      "step": 5620
    },
    {
      "epoch": 1.563137799375217,
      "grad_norm": 4.28125,
      "learning_rate": 4.3654540405442935e-05,
      "loss": 3.8578,
      "step": 5630
    },
    {
      "epoch": 1.5659146129816035,
      "grad_norm": 5.125,
      "learning_rate": 4.337683976673147e-05,
      "loss": 3.9742,
      "step": 5640
    },
    {
      "epoch": 1.5686914265879903,
      "grad_norm": 4.28125,
      "learning_rate": 4.309913912801999e-05,
      "loss": 4.0016,
      "step": 5650
    },
    {
      "epoch": 1.571468240194377,
      "grad_norm": 4.96875,
      "learning_rate": 4.282143848930853e-05,
      "loss": 4.1387,
      "step": 5660
    },
    {
      "epoch": 1.5742450538007637,
      "grad_norm": 5.25,
      "learning_rate": 4.254373785059706e-05,
      "loss": 3.9707,
      "step": 5670
    },
    {
      "epoch": 1.5770218674071503,
      "grad_norm": 4.75,
      "learning_rate": 4.226603721188559e-05,
      "loss": 4.116,
      "step": 5680
    },
    {
      "epoch": 1.579798681013537,
      "grad_norm": 4.65625,
      "learning_rate": 4.198833657317412e-05,
      "loss": 3.9613,
      "step": 5690
    },
    {
      "epoch": 1.5825754946199235,
      "grad_norm": 4.09375,
      "learning_rate": 4.171063593446265e-05,
      "loss": 3.7961,
      "step": 5700
    },
    {
      "epoch": 1.5853523082263103,
      "grad_norm": 3.6875,
      "learning_rate": 4.143293529575118e-05,
      "loss": 3.9332,
      "step": 5710
    },
    {
      "epoch": 1.588129121832697,
      "grad_norm": 4.03125,
      "learning_rate": 4.115523465703972e-05,
      "loss": 3.9309,
      "step": 5720
    },
    {
      "epoch": 1.5909059354390838,
      "grad_norm": 4.1875,
      "learning_rate": 4.0877534018328244e-05,
      "loss": 3.998,
      "step": 5730
    },
    {
      "epoch": 1.5936827490454704,
      "grad_norm": 5.21875,
      "learning_rate": 4.0599833379616776e-05,
      "loss": 4.0914,
      "step": 5740
    },
    {
      "epoch": 1.596459562651857,
      "grad_norm": 3.953125,
      "learning_rate": 4.032213274090531e-05,
      "loss": 4.0176,
      "step": 5750
    },
    {
      "epoch": 1.5992363762582436,
      "grad_norm": 4.15625,
      "learning_rate": 4.0044432102193834e-05,
      "loss": 3.7969,
      "step": 5760
    },
    {
      "epoch": 1.6020131898646304,
      "grad_norm": 5.40625,
      "learning_rate": 3.976673146348237e-05,
      "loss": 4.0137,
      "step": 5770
    },
    {
      "epoch": 1.604790003471017,
      "grad_norm": 4.84375,
      "learning_rate": 3.94890308247709e-05,
      "loss": 3.8105,
      "step": 5780
    },
    {
      "epoch": 1.6075668170774038,
      "grad_norm": 4.71875,
      "learning_rate": 3.921133018605943e-05,
      "loss": 3.9555,
      "step": 5790
    },
    {
      "epoch": 1.6103436306837904,
      "grad_norm": 4.84375,
      "learning_rate": 3.893362954734796e-05,
      "loss": 3.9824,
      "step": 5800
    },
    {
      "epoch": 1.613120444290177,
      "grad_norm": 4.4375,
      "learning_rate": 3.865592890863649e-05,
      "loss": 3.8855,
      "step": 5810
    },
    {
      "epoch": 1.6158972578965636,
      "grad_norm": 4.875,
      "learning_rate": 3.837822826992502e-05,
      "loss": 3.9141,
      "step": 5820
    },
    {
      "epoch": 1.6186740715029504,
      "grad_norm": 5.3125,
      "learning_rate": 3.810052763121356e-05,
      "loss": 3.9941,
      "step": 5830
    },
    {
      "epoch": 1.621450885109337,
      "grad_norm": 5.125,
      "learning_rate": 3.7822826992502085e-05,
      "loss": 3.8953,
      "step": 5840
    },
    {
      "epoch": 1.6242276987157238,
      "grad_norm": 4.875,
      "learning_rate": 3.754512635379062e-05,
      "loss": 4.0797,
      "step": 5850
    },
    {
      "epoch": 1.6270045123221104,
      "grad_norm": 4.46875,
      "learning_rate": 3.726742571507915e-05,
      "loss": 4.0781,
      "step": 5860
    },
    {
      "epoch": 1.629781325928497,
      "grad_norm": 4.5,
      "learning_rate": 3.6989725076367675e-05,
      "loss": 3.9809,
      "step": 5870
    },
    {
      "epoch": 1.6325581395348836,
      "grad_norm": 4.34375,
      "learning_rate": 3.671202443765621e-05,
      "loss": 3.8758,
      "step": 5880
    },
    {
      "epoch": 1.6353349531412704,
      "grad_norm": 4.125,
      "learning_rate": 3.643432379894474e-05,
      "loss": 4.1348,
      "step": 5890
    },
    {
      "epoch": 1.638111766747657,
      "grad_norm": 8.4375,
      "learning_rate": 3.615662316023327e-05,
      "loss": 3.9887,
      "step": 5900
    },
    {
      "epoch": 1.6408885803540438,
      "grad_norm": 6.0625,
      "learning_rate": 3.5878922521521804e-05,
      "loss": 4.0094,
      "step": 5910
    },
    {
      "epoch": 1.6436653939604304,
      "grad_norm": 4.5625,
      "learning_rate": 3.560122188281033e-05,
      "loss": 3.9477,
      "step": 5920
    },
    {
      "epoch": 1.646442207566817,
      "grad_norm": 4.0625,
      "learning_rate": 3.532352124409886e-05,
      "loss": 4.0238,
      "step": 5930
    },
    {
      "epoch": 1.6492190211732036,
      "grad_norm": 5.1875,
      "learning_rate": 3.5045820605387394e-05,
      "loss": 3.7988,
      "step": 5940
    },
    {
      "epoch": 1.6519958347795904,
      "grad_norm": 5.0,
      "learning_rate": 3.4768119966675926e-05,
      "loss": 3.9777,
      "step": 5950
    },
    {
      "epoch": 1.654772648385977,
      "grad_norm": 5.1875,
      "learning_rate": 3.449041932796446e-05,
      "loss": 3.877,
      "step": 5960
    },
    {
      "epoch": 1.6575494619923639,
      "grad_norm": 4.34375,
      "learning_rate": 3.4212718689252984e-05,
      "loss": 4.1965,
      "step": 5970
    },
    {
      "epoch": 1.6603262755987505,
      "grad_norm": 4.90625,
      "learning_rate": 3.3935018050541516e-05,
      "loss": 3.934,
      "step": 5980
    },
    {
      "epoch": 1.663103089205137,
      "grad_norm": 5.4375,
      "learning_rate": 3.365731741183005e-05,
      "loss": 3.834,
      "step": 5990
    },
    {
      "epoch": 1.6658799028115236,
      "grad_norm": 5.0625,
      "learning_rate": 3.3379616773118574e-05,
      "loss": 3.916,
      "step": 6000
    },
    {
      "epoch": 1.6686567164179105,
      "grad_norm": 5.0,
      "learning_rate": 3.310191613440711e-05,
      "loss": 3.9922,
      "step": 6010
    },
    {
      "epoch": 1.671433530024297,
      "grad_norm": 9.3125,
      "learning_rate": 3.2824215495695645e-05,
      "loss": 3.8664,
      "step": 6020
    },
    {
      "epoch": 1.6742103436306839,
      "grad_norm": 4.6875,
      "learning_rate": 3.254651485698417e-05,
      "loss": 3.8664,
      "step": 6030
    },
    {
      "epoch": 1.6769871572370705,
      "grad_norm": 5.59375,
      "learning_rate": 3.22688142182727e-05,
      "loss": 3.9316,
      "step": 6040
    },
    {
      "epoch": 1.679763970843457,
      "grad_norm": 7.03125,
      "learning_rate": 3.1991113579561235e-05,
      "loss": 3.9723,
      "step": 6050
    },
    {
      "epoch": 1.6825407844498437,
      "grad_norm": 4.5,
      "learning_rate": 3.171341294084977e-05,
      "loss": 4.0746,
      "step": 6060
    },
    {
      "epoch": 1.6853175980562305,
      "grad_norm": 4.875,
      "learning_rate": 3.14357123021383e-05,
      "loss": 3.8914,
      "step": 6070
    },
    {
      "epoch": 1.6880944116626173,
      "grad_norm": 4.34375,
      "learning_rate": 3.1158011663426825e-05,
      "loss": 3.7867,
      "step": 6080
    },
    {
      "epoch": 1.690871225269004,
      "grad_norm": 4.5625,
      "learning_rate": 3.088031102471536e-05,
      "loss": 3.8383,
      "step": 6090
    },
    {
      "epoch": 1.6936480388753905,
      "grad_norm": 5.0,
      "learning_rate": 3.060261038600389e-05,
      "loss": 3.9082,
      "step": 6100
    },
    {
      "epoch": 1.696424852481777,
      "grad_norm": 3.734375,
      "learning_rate": 3.032490974729242e-05,
      "loss": 4.0148,
      "step": 6110
    },
    {
      "epoch": 1.6992016660881637,
      "grad_norm": 5.65625,
      "learning_rate": 3.0047209108580954e-05,
      "loss": 3.8762,
      "step": 6120
    },
    {
      "epoch": 1.7019784796945505,
      "grad_norm": 5.8125,
      "learning_rate": 2.9769508469869483e-05,
      "loss": 4.1016,
      "step": 6130
    },
    {
      "epoch": 1.7047552933009373,
      "grad_norm": 4.6875,
      "learning_rate": 2.9491807831158015e-05,
      "loss": 4.0535,
      "step": 6140
    },
    {
      "epoch": 1.707532106907324,
      "grad_norm": 5.1875,
      "learning_rate": 2.9214107192446544e-05,
      "loss": 4.0285,
      "step": 6150
    },
    {
      "epoch": 1.7103089205137105,
      "grad_norm": 5.34375,
      "learning_rate": 2.8936406553735073e-05,
      "loss": 4.0793,
      "step": 6160
    },
    {
      "epoch": 1.7130857341200971,
      "grad_norm": 4.5,
      "learning_rate": 2.8658705915023605e-05,
      "loss": 3.9406,
      "step": 6170
    },
    {
      "epoch": 1.7158625477264837,
      "grad_norm": 5.40625,
      "learning_rate": 2.838100527631214e-05,
      "loss": 4.0133,
      "step": 6180
    },
    {
      "epoch": 1.7186393613328705,
      "grad_norm": 5.1875,
      "learning_rate": 2.810330463760067e-05,
      "loss": 3.716,
      "step": 6190
    },
    {
      "epoch": 1.7214161749392574,
      "grad_norm": 4.75,
      "learning_rate": 2.78256039988892e-05,
      "loss": 4.1039,
      "step": 6200
    },
    {
      "epoch": 1.724192988545644,
      "grad_norm": 4.28125,
      "learning_rate": 2.754790336017773e-05,
      "loss": 4.0488,
      "step": 6210
    },
    {
      "epoch": 1.7269698021520306,
      "grad_norm": 5.59375,
      "learning_rate": 2.727020272146626e-05,
      "loss": 3.9609,
      "step": 6220
    },
    {
      "epoch": 1.7297466157584171,
      "grad_norm": 5.03125,
      "learning_rate": 2.699250208275479e-05,
      "loss": 3.9992,
      "step": 6230
    },
    {
      "epoch": 1.7325234293648037,
      "grad_norm": 4.34375,
      "learning_rate": 2.6714801444043324e-05,
      "loss": 4.1344,
      "step": 6240
    },
    {
      "epoch": 1.7353002429711906,
      "grad_norm": 3.578125,
      "learning_rate": 2.6437100805331853e-05,
      "loss": 3.6336,
      "step": 6250
    },
    {
      "epoch": 1.7380770565775774,
      "grad_norm": 4.5625,
      "learning_rate": 2.6159400166620385e-05,
      "loss": 4.0238,
      "step": 6260
    },
    {
      "epoch": 1.740853870183964,
      "grad_norm": 4.6875,
      "learning_rate": 2.5881699527908914e-05,
      "loss": 3.9074,
      "step": 6270
    },
    {
      "epoch": 1.7436306837903506,
      "grad_norm": 4.59375,
      "learning_rate": 2.5603998889197443e-05,
      "loss": 4.0105,
      "step": 6280
    },
    {
      "epoch": 1.7464074973967372,
      "grad_norm": 6.40625,
      "learning_rate": 2.5326298250485975e-05,
      "loss": 3.9016,
      "step": 6290
    },
    {
      "epoch": 1.7491843110031238,
      "grad_norm": 4.375,
      "learning_rate": 2.504859761177451e-05,
      "loss": 3.9875,
      "step": 6300
    },
    {
      "epoch": 1.7519611246095106,
      "grad_norm": 4.1875,
      "learning_rate": 2.477089697306304e-05,
      "loss": 3.9,
      "step": 6310
    },
    {
      "epoch": 1.7547379382158974,
      "grad_norm": 5.40625,
      "learning_rate": 2.449319633435157e-05,
      "loss": 3.9445,
      "step": 6320
    },
    {
      "epoch": 1.757514751822284,
      "grad_norm": 5.75,
      "learning_rate": 2.42154956956401e-05,
      "loss": 4.0094,
      "step": 6330
    },
    {
      "epoch": 1.7602915654286706,
      "grad_norm": 4.65625,
      "learning_rate": 2.3937795056928633e-05,
      "loss": 3.902,
      "step": 6340
    },
    {
      "epoch": 1.7630683790350572,
      "grad_norm": 5.34375,
      "learning_rate": 2.3660094418217162e-05,
      "loss": 3.825,
      "step": 6350
    },
    {
      "epoch": 1.7658451926414438,
      "grad_norm": 4.34375,
      "learning_rate": 2.3382393779505694e-05,
      "loss": 4.0273,
      "step": 6360
    },
    {
      "epoch": 1.7686220062478306,
      "grad_norm": 4.875,
      "learning_rate": 2.3104693140794227e-05,
      "loss": 4.0965,
      "step": 6370
    },
    {
      "epoch": 1.7713988198542174,
      "grad_norm": 4.71875,
      "learning_rate": 2.2826992502082755e-05,
      "loss": 3.9816,
      "step": 6380
    },
    {
      "epoch": 1.774175633460604,
      "grad_norm": 4.21875,
      "learning_rate": 2.2549291863371284e-05,
      "loss": 4.0395,
      "step": 6390
    },
    {
      "epoch": 1.7769524470669906,
      "grad_norm": 4.6875,
      "learning_rate": 2.227159122465982e-05,
      "loss": 3.9066,
      "step": 6400
    },
    {
      "epoch": 1.7797292606733772,
      "grad_norm": 4.8125,
      "learning_rate": 2.199389058594835e-05,
      "loss": 3.9566,
      "step": 6410
    },
    {
      "epoch": 1.7825060742797638,
      "grad_norm": 4.6875,
      "learning_rate": 2.1716189947236878e-05,
      "loss": 4.016,
      "step": 6420
    },
    {
      "epoch": 1.7852828878861506,
      "grad_norm": 4.59375,
      "learning_rate": 2.143848930852541e-05,
      "loss": 4.0602,
      "step": 6430
    },
    {
      "epoch": 1.7880597014925375,
      "grad_norm": 4.375,
      "learning_rate": 2.1160788669813942e-05,
      "loss": 3.8461,
      "step": 6440
    },
    {
      "epoch": 1.790836515098924,
      "grad_norm": 5.03125,
      "learning_rate": 2.088308803110247e-05,
      "loss": 4.1254,
      "step": 6450
    },
    {
      "epoch": 1.7936133287053106,
      "grad_norm": 5.34375,
      "learning_rate": 2.0605387392391003e-05,
      "loss": 4.0262,
      "step": 6460
    },
    {
      "epoch": 1.7963901423116972,
      "grad_norm": 3.96875,
      "learning_rate": 2.0327686753679535e-05,
      "loss": 3.991,
      "step": 6470
    },
    {
      "epoch": 1.7991669559180838,
      "grad_norm": 5.625,
      "learning_rate": 2.0049986114968064e-05,
      "loss": 3.8824,
      "step": 6480
    },
    {
      "epoch": 1.8019437695244707,
      "grad_norm": 5.0,
      "learning_rate": 1.9772285476256597e-05,
      "loss": 3.9836,
      "step": 6490
    },
    {
      "epoch": 1.8047205831308575,
      "grad_norm": 5.09375,
      "learning_rate": 1.9494584837545125e-05,
      "loss": 4.0379,
      "step": 6500
    },
    {
      "epoch": 1.807497396737244,
      "grad_norm": 4.3125,
      "learning_rate": 1.921688419883366e-05,
      "loss": 3.9641,
      "step": 6510
    },
    {
      "epoch": 1.8102742103436307,
      "grad_norm": 3.875,
      "learning_rate": 1.893918356012219e-05,
      "loss": 3.9883,
      "step": 6520
    },
    {
      "epoch": 1.8130510239500173,
      "grad_norm": 3.78125,
      "learning_rate": 1.866148292141072e-05,
      "loss": 3.8855,
      "step": 6530
    },
    {
      "epoch": 1.8158278375564039,
      "grad_norm": 4.1875,
      "learning_rate": 1.838378228269925e-05,
      "loss": 3.952,
      "step": 6540
    },
    {
      "epoch": 1.8186046511627907,
      "grad_norm": 5.125,
      "learning_rate": 1.8106081643987783e-05,
      "loss": 3.8828,
      "step": 6550
    },
    {
      "epoch": 1.8213814647691775,
      "grad_norm": 4.40625,
      "learning_rate": 1.7828381005276312e-05,
      "loss": 3.8172,
      "step": 6560
    },
    {
      "epoch": 1.824158278375564,
      "grad_norm": 3.96875,
      "learning_rate": 1.7550680366564844e-05,
      "loss": 3.934,
      "step": 6570
    },
    {
      "epoch": 1.8269350919819507,
      "grad_norm": 4.8125,
      "learning_rate": 1.7272979727853377e-05,
      "loss": 4.1625,
      "step": 6580
    },
    {
      "epoch": 1.8297119055883373,
      "grad_norm": 4.90625,
      "learning_rate": 1.6995279089141906e-05,
      "loss": 4.016,
      "step": 6590
    },
    {
      "epoch": 1.832488719194724,
      "grad_norm": 3.5625,
      "learning_rate": 1.6717578450430438e-05,
      "loss": 4.0656,
      "step": 6600
    },
    {
      "epoch": 1.8352655328011107,
      "grad_norm": 4.5,
      "learning_rate": 1.6439877811718967e-05,
      "loss": 3.8711,
      "step": 6610
    },
    {
      "epoch": 1.8380423464074975,
      "grad_norm": 3.828125,
      "learning_rate": 1.61621771730075e-05,
      "loss": 3.8273,
      "step": 6620
    },
    {
      "epoch": 1.8408191600138841,
      "grad_norm": 4.25,
      "learning_rate": 1.588447653429603e-05,
      "loss": 3.9297,
      "step": 6630
    },
    {
      "epoch": 1.8435959736202707,
      "grad_norm": 5.3125,
      "learning_rate": 1.560677589558456e-05,
      "loss": 4.0039,
      "step": 6640
    },
    {
      "epoch": 1.8463727872266573,
      "grad_norm": 4.90625,
      "learning_rate": 1.532907525687309e-05,
      "loss": 3.9941,
      "step": 6650
    },
    {
      "epoch": 1.849149600833044,
      "grad_norm": 5.34375,
      "learning_rate": 1.5051374618161623e-05,
      "loss": 3.9211,
      "step": 6660
    },
    {
      "epoch": 1.8519264144394307,
      "grad_norm": 5.6875,
      "learning_rate": 1.4773673979450153e-05,
      "loss": 3.7375,
      "step": 6670
    },
    {
      "epoch": 1.8547032280458176,
      "grad_norm": 5.3125,
      "learning_rate": 1.4495973340738684e-05,
      "loss": 4.3059,
      "step": 6680
    },
    {
      "epoch": 1.8574800416522042,
      "grad_norm": 4.75,
      "learning_rate": 1.4218272702027216e-05,
      "loss": 3.8711,
      "step": 6690
    },
    {
      "epoch": 1.8602568552585907,
      "grad_norm": 4.375,
      "learning_rate": 1.3940572063315747e-05,
      "loss": 4.0676,
      "step": 6700
    },
    {
      "epoch": 1.8630336688649773,
      "grad_norm": 5.125,
      "learning_rate": 1.3662871424604276e-05,
      "loss": 4.0547,
      "step": 6710
    },
    {
      "epoch": 1.8658104824713642,
      "grad_norm": 4.46875,
      "learning_rate": 1.338517078589281e-05,
      "loss": 3.85,
      "step": 6720
    },
    {
      "epoch": 1.8685872960777508,
      "grad_norm": 4.125,
      "learning_rate": 1.3107470147181338e-05,
      "loss": 4.0215,
      "step": 6730
    },
    {
      "epoch": 1.8713641096841376,
      "grad_norm": 4.78125,
      "learning_rate": 1.2829769508469869e-05,
      "loss": 4.059,
      "step": 6740
    },
    {
      "epoch": 1.8741409232905242,
      "grad_norm": 4.5625,
      "learning_rate": 1.2552068869758401e-05,
      "loss": 3.9617,
      "step": 6750
    },
    {
      "epoch": 1.8769177368969108,
      "grad_norm": 4.71875,
      "learning_rate": 1.2274368231046932e-05,
      "loss": 3.9676,
      "step": 6760
    },
    {
      "epoch": 1.8796945505032974,
      "grad_norm": 3.71875,
      "learning_rate": 1.1996667592335462e-05,
      "loss": 3.9266,
      "step": 6770
    },
    {
      "epoch": 1.8824713641096842,
      "grad_norm": 5.15625,
      "learning_rate": 1.1718966953623995e-05,
      "loss": 4.098,
      "step": 6780
    },
    {
      "epoch": 1.8852481777160708,
      "grad_norm": 4.4375,
      "learning_rate": 1.1441266314912525e-05,
      "loss": 3.9793,
      "step": 6790
    },
    {
      "epoch": 1.8880249913224576,
      "grad_norm": 4.53125,
      "learning_rate": 1.1163565676201056e-05,
      "loss": 3.9531,
      "step": 6800
    },
    {
      "epoch": 1.8908018049288442,
      "grad_norm": 4.3125,
      "learning_rate": 1.0885865037489586e-05,
      "loss": 3.8176,
      "step": 6810
    },
    {
      "epoch": 1.8935786185352308,
      "grad_norm": 7.25,
      "learning_rate": 1.0608164398778117e-05,
      "loss": 3.9941,
      "step": 6820
    },
    {
      "epoch": 1.8963554321416174,
      "grad_norm": 6.25,
      "learning_rate": 1.0330463760066649e-05,
      "loss": 3.9641,
      "step": 6830
    },
    {
      "epoch": 1.8991322457480042,
      "grad_norm": 5.03125,
      "learning_rate": 1.005276312135518e-05,
      "loss": 3.9547,
      "step": 6840
    },
    {
      "epoch": 1.9019090593543908,
      "grad_norm": 4.59375,
      "learning_rate": 9.775062482643712e-06,
      "loss": 3.9266,
      "step": 6850
    },
    {
      "epoch": 1.9046858729607776,
      "grad_norm": 5.46875,
      "learning_rate": 9.49736184393224e-06,
      "loss": 3.8602,
      "step": 6860
    },
    {
      "epoch": 1.9074626865671642,
      "grad_norm": 6.0625,
      "learning_rate": 9.219661205220773e-06,
      "loss": 3.9301,
      "step": 6870
    },
    {
      "epoch": 1.9102395001735508,
      "grad_norm": 3.875,
      "learning_rate": 8.941960566509304e-06,
      "loss": 4.0578,
      "step": 6880
    },
    {
      "epoch": 1.9130163137799374,
      "grad_norm": 4.71875,
      "learning_rate": 8.664259927797834e-06,
      "loss": 4.1211,
      "step": 6890
    },
    {
      "epoch": 1.9157931273863242,
      "grad_norm": 4.59375,
      "learning_rate": 8.386559289086365e-06,
      "loss": 3.9566,
      "step": 6900
    },
    {
      "epoch": 1.9185699409927108,
      "grad_norm": 4.9375,
      "learning_rate": 8.108858650374897e-06,
      "loss": 4.0379,
      "step": 6910
    },
    {
      "epoch": 1.9213467545990977,
      "grad_norm": 5.53125,
      "learning_rate": 7.831158011663427e-06,
      "loss": 4.0148,
      "step": 6920
    },
    {
      "epoch": 1.9241235682054842,
      "grad_norm": 4.4375,
      "learning_rate": 7.553457372951958e-06,
      "loss": 3.9762,
      "step": 6930
    },
    {
      "epoch": 1.9269003818118708,
      "grad_norm": 4.71875,
      "learning_rate": 7.275756734240489e-06,
      "loss": 3.8113,
      "step": 6940
    },
    {
      "epoch": 1.9296771954182574,
      "grad_norm": 4.6875,
      "learning_rate": 6.998056095529019e-06,
      "loss": 3.9508,
      "step": 6950
    },
    {
      "epoch": 1.9324540090246443,
      "grad_norm": 5.59375,
      "learning_rate": 6.7203554568175505e-06,
      "loss": 3.943,
      "step": 6960
    },
    {
      "epoch": 1.9352308226310309,
      "grad_norm": 4.375,
      "learning_rate": 6.442654818106082e-06,
      "loss": 3.9285,
      "step": 6970
    },
    {
      "epoch": 1.9380076362374177,
      "grad_norm": 5.625,
      "learning_rate": 6.164954179394613e-06,
      "loss": 3.9371,
      "step": 6980
    },
    {
      "epoch": 1.9407844498438043,
      "grad_norm": 4.0625,
      "learning_rate": 5.887253540683144e-06,
      "loss": 3.9902,
      "step": 6990
    },
    {
      "epoch": 1.9435612634501909,
      "grad_norm": 3.921875,
      "learning_rate": 5.609552901971675e-06,
      "loss": 4.1434,
      "step": 7000
    },
    {
      "epoch": 1.9463380770565775,
      "grad_norm": 4.5,
      "learning_rate": 5.331852263260206e-06,
      "loss": 4.0547,
      "step": 7010
    },
    {
      "epoch": 1.9491148906629643,
      "grad_norm": 4.59375,
      "learning_rate": 5.054151624548736e-06,
      "loss": 3.9574,
      "step": 7020
    },
    {
      "epoch": 1.9518917042693509,
      "grad_norm": 4.8125,
      "learning_rate": 4.776450985837268e-06,
      "loss": 3.9137,
      "step": 7030
    },
    {
      "epoch": 1.9546685178757377,
      "grad_norm": 4.71875,
      "learning_rate": 4.498750347125798e-06,
      "loss": 3.8344,
      "step": 7040
    },
    {
      "epoch": 1.9574453314821243,
      "grad_norm": 4.65625,
      "learning_rate": 4.22104970841433e-06,
      "loss": 3.8809,
      "step": 7050
    },
    {
      "epoch": 1.960222145088511,
      "grad_norm": 7.53125,
      "learning_rate": 3.94334906970286e-06,
      "loss": 4.0004,
      "step": 7060
    },
    {
      "epoch": 1.9629989586948975,
      "grad_norm": 5.1875,
      "learning_rate": 3.6656484309913913e-06,
      "loss": 3.982,
      "step": 7070
    },
    {
      "epoch": 1.9657757723012843,
      "grad_norm": 4.1875,
      "learning_rate": 3.3879477922799227e-06,
      "loss": 3.9211,
      "step": 7080
    },
    {
      "epoch": 1.968552585907671,
      "grad_norm": 4.90625,
      "learning_rate": 3.1102471535684533e-06,
      "loss": 3.918,
      "step": 7090
    },
    {
      "epoch": 1.9713293995140577,
      "grad_norm": 4.875,
      "learning_rate": 2.8325465148569843e-06,
      "loss": 4.0789,
      "step": 7100
    },
    {
      "epoch": 1.9741062131204443,
      "grad_norm": 9.5625,
      "learning_rate": 2.5548458761455152e-06,
      "loss": 3.8953,
      "step": 7110
    },
    {
      "epoch": 1.976883026726831,
      "grad_norm": 4.5625,
      "learning_rate": 2.2771452374340462e-06,
      "loss": 3.7613,
      "step": 7120
    },
    {
      "epoch": 1.9796598403332175,
      "grad_norm": 4.78125,
      "learning_rate": 1.999444598722577e-06,
      "loss": 4.1168,
      "step": 7130
    },
    {
      "epoch": 1.9824366539396043,
      "grad_norm": 4.65625,
      "learning_rate": 1.7217439600111082e-06,
      "loss": 3.9242,
      "step": 7140
    },
    {
      "epoch": 1.985213467545991,
      "grad_norm": 4.28125,
      "learning_rate": 1.4440433212996392e-06,
      "loss": 3.9906,
      "step": 7150
    },
    {
      "epoch": 1.9879902811523777,
      "grad_norm": 5.0625,
      "learning_rate": 1.16634268258817e-06,
      "loss": 3.8508,
      "step": 7160
    },
    {
      "epoch": 1.9907670947587643,
      "grad_norm": 4.90625,
      "learning_rate": 8.88642043876701e-07,
      "loss": 4.1523,
      "step": 7170
    },
    {
      "epoch": 1.993543908365151,
      "grad_norm": 5.0625,
      "learning_rate": 6.10941405165232e-07,
      "loss": 4.1375,
      "step": 7180
    },
    {
      "epoch": 1.9963207219715375,
      "grad_norm": 4.71875,
      "learning_rate": 3.3324076645376287e-07,
      "loss": 3.952,
      "step": 7190
    },
    {
      "epoch": 1.9990975355779244,
      "grad_norm": 4.21875,
      "learning_rate": 5.554012774229381e-08,
      "loss": 3.9309,
      "step": 7200
    }
  ],
  "logging_steps": 10,
  "max_steps": 7202,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.486423658215834e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
